{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3e865f-3694-4e7f-b2ba-1e24d7a00d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "In year 2002\n",
    "-------------\n",
    "Desktop S/W  VB+MSAccess - billing,...,.....\n",
    "|\n",
    "In year 2025\n",
    "-------------\n",
    "S/W + AI\n",
    "      ---\n",
    "        |->QA - chat ...\n",
    "        |-> .. ....\n",
    "        |-> ...\n",
    "OpenAI - 7b\n",
    "Groq   - 18b\n",
    "Huggingface - 3b\n",
    "Ollama - 2b\n",
    "\n",
    "    pA - list of birds - 100books    Vs   pB - list of birds - 200books\n",
    "\n",
    "    pB + 5more books \n",
    "         (mydata-business)\n",
    "===============================================================================================  \n",
    "Recap\n",
    "=====\n",
    "DAY1 - Tokenization - stem/lemma - vector \n",
    "                              |->onehot ; bow ; TF ; word2vector  ->transformer \n",
    "                                                    -------------\n",
    "DAY2 - Langchain - Framework \n",
    "                     |-> llm -- genAI -- applications\n",
    "          |\n",
    "          |->data loader \n",
    "              - TextLoader \n",
    "              - PyPDFLoader\n",
    "              - WebbaseLoader\n",
    "              - ...\n",
    "           =======================// metadata + page_content(Actual data) //\n",
    "          |-->Document(...) --> //metadata + page_content //\n",
    "\n",
    "          |-->Split - data into mutiple chunks  [][][][]\n",
    "          |-->Embedding -->embedded_object\n",
    "          |--> VectorDB - stores the splitted data into VectorDB\n",
    "          |--> similarity\n",
    "\n",
    " Ollama model ->downloaded to local system --> There is NO API ; NO Token \n",
    "            |->act as a os service - running on local system\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------\n",
    "Linux/mac os --> vi .env\n",
    "windows ==> Gitbash => vi .env {Enter}\n",
    "\n",
    "\t\t\tpress i - insert mode\n",
    "\n",
    "\t\t\tHF_TOKEN=\"hf...M\"\n",
    "\t\t\t       ----//there is no space in = LHS,RHS\n",
    "\t\t\tpress ESC \n",
    "\t\t\t:wq <== save and quit\n",
    "\t\t\t\n",
    "\t\t\tcat .env\n",
    "\n",
    "project/\n",
    "      |___ p1.py\n",
    "      |___ .env \n",
    "\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv() ->True \n",
    "\n",
    " Vs\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv() ->False  <-------- .env file is not exists\n",
    "\n",
    "import os\n",
    "os.getenv "
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c27e5da-5af0-4749-8eaa-17cb33137b1d",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c83619a-9dc3-4850-b5b2-69210a398427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\Users\\\\karth'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['HOMEPATH'] # this is dict style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2100c4d0-3b7d-44a7-aa9c-a32c51df8017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\Users\\\\karth'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('HOMEPATH') # this is object.method(Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e9441f5-4fb5-4663-ad2a-fab9d2f8fc82",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5bf8908-9dd5-48eb-94e7-78c819f96b18",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "os.chdir(\"C:/Users/karth\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70366100-3d23-4f79-bce2-5984d32986da",
   "metadata": {},
   "source": [
    "os.getenv('HF_TOKEN') # -->empty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b5f1a1d-168f-4da9-a042-70c4749ba818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f8ac66f-c6c4-4f39-84e1-2609e7db90ef",
   "metadata": {},
   "source": [
    "os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14f422ea-15c3-470b-8b2c-ed06d94c445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "466d8212-e30c-43e2-8a0b-9c3b093e87a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37ca8b3d-1399-45fb-9a6d-f8e8794d1f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_huggingface.embeddings.huggingface.HuggingFaceEmbeddings'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "print(HuggingFaceEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f715a8e-bd20-447f-8586-0cd08d394f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80dde93c-72d2-4029-ab9f-5ed1ae2ae4c2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.0627717599272728,\n",
       " 0.05495879426598549,\n",
       " 0.052164845168590546,\n",
       " 0.0857900083065033,\n",
       " -0.0827488973736763,\n",
       " -0.07457297295331955,\n",
       " 0.06855470687150955,\n",
       " 0.018396394327282906,\n",
       " -0.08201129734516144,\n",
       " -0.03738484904170036,\n",
       " 0.012124893255531788,\n",
       " 0.0035183229483664036,\n",
       " -0.004134293645620346,\n",
       " -0.04378441721200943,\n",
       " 0.021807299926877022,\n",
       " -0.00510272104293108,\n",
       " 0.019546564668416977,\n",
       " -0.0423487089574337,\n",
       " -0.11035966873168945,\n",
       " 0.005424527917057276,\n",
       " -0.055734783411026,\n",
       " 0.02805238962173462,\n",
       " -0.02315872348845005,\n",
       " 0.028481375426054,\n",
       " -0.05370955914258957,\n",
       " -0.052601538598537445,\n",
       " 0.03393925353884697,\n",
       " 0.045388635247945786,\n",
       " 0.023718422278761864,\n",
       " -0.07312081754207611,\n",
       " 0.05477774888277054,\n",
       " 0.0170473363250494,\n",
       " 0.08136039972305298,\n",
       " -0.0028627056162804365,\n",
       " 0.011958062648773193,\n",
       " 0.07355856150388718,\n",
       " -0.09423744678497314,\n",
       " -0.0813620463013649,\n",
       " 0.04001539200544357,\n",
       " 0.0006921913009136915,\n",
       " -0.01339334063231945,\n",
       " -0.05453811213374138,\n",
       " 0.005151401273906231,\n",
       " -0.026139769703149796,\n",
       " 0.0368071012198925,\n",
       " -0.03395958989858627,\n",
       " 0.021093212068080902,\n",
       " 0.055948879569768906,\n",
       " 0.05778130516409874,\n",
       " -0.0054183765314519405,\n",
       " -0.06841319799423218,\n",
       " -0.09023699164390564,\n",
       " -0.04286673292517662,\n",
       " 0.023652682080864906,\n",
       " 0.12149356305599213,\n",
       " 0.032392844557762146,\n",
       " -0.02265944518148899,\n",
       " -0.02328176610171795,\n",
       " 0.04886598512530327,\n",
       " -0.059351544827222824,\n",
       " -0.03406606614589691,\n",
       " 0.03596258535981178,\n",
       " -0.08115777373313904,\n",
       " -0.021979473531246185,\n",
       " 0.013187813572585583,\n",
       " -0.04585552215576172,\n",
       " -0.07041982561349869,\n",
       " -0.05260329321026802,\n",
       " -0.04802926629781723,\n",
       " -0.07454610615968704,\n",
       " -0.028731804341077805,\n",
       " 0.012831883504986763,\n",
       " -0.042921729385852814,\n",
       " 0.0053663537837564945,\n",
       " -0.03909760341048241,\n",
       " 0.015919268131256104,\n",
       " 0.020243527367711067,\n",
       " 0.0052666617557406425,\n",
       " 0.009581327438354492,\n",
       " -0.0047676158137619495,\n",
       " 0.04808773100376129,\n",
       " -0.07309659570455551,\n",
       " -0.05034444108605385,\n",
       " 0.009653042070567608,\n",
       " 0.013496600091457367,\n",
       " 0.00043035211274400353,\n",
       " 0.019725199788808823,\n",
       " 0.062495484948158264,\n",
       " -0.01822715625166893,\n",
       " 0.03109716810286045,\n",
       " -0.08885394781827927,\n",
       " 0.04813242703676224,\n",
       " 0.026074277237057686,\n",
       " -0.011494041420519352,\n",
       " -0.09936171025037766,\n",
       " -0.027338232845067978,\n",
       " 0.07621528953313828,\n",
       " -0.002096540527418256,\n",
       " -0.12358873337507248,\n",
       " 0.29806533455848694,\n",
       " 0.05630640685558319,\n",
       " 0.0784800797700882,\n",
       " 0.01104352530092001,\n",
       " 0.052400071173906326,\n",
       " -0.0011880238307639956,\n",
       " 0.000965684128459543,\n",
       " -0.05432729423046112,\n",
       " 0.02676473930478096,\n",
       " -0.012194258160889149,\n",
       " 0.010717053897678852,\n",
       " -3.707817450049333e-05,\n",
       " -0.035565707832574844,\n",
       " -0.03208553418517113,\n",
       " 0.016168801113963127,\n",
       " 0.08674969524145126,\n",
       " 0.018139464780688286,\n",
       " -0.01101755816489458,\n",
       " 0.04974030703306198,\n",
       " 0.02258104830980301,\n",
       " -2.2351243387674913e-05,\n",
       " 0.014939067885279655,\n",
       " -0.011176216416060925,\n",
       " 0.005659336689859629,\n",
       " -0.004114812705665827,\n",
       " 0.003182594431564212,\n",
       " 0.02386500872671604,\n",
       " 0.01805107854306698,\n",
       " -5.850476159002819e-33,\n",
       " 0.05610707402229309,\n",
       " -0.03449849784374237,\n",
       " 0.033111825585365295,\n",
       " 0.1677229106426239,\n",
       " -0.031038135290145874,\n",
       " -0.0048353467136621475,\n",
       " -0.06106499955058098,\n",
       " -0.06271100044250488,\n",
       " 0.027426589280366898,\n",
       " 0.06364015489816666,\n",
       " 0.043340425938367844,\n",
       " 0.06082936003804207,\n",
       " -0.018160779029130936,\n",
       " 0.04280414804816246,\n",
       " 0.019024066627025604,\n",
       " 0.08797787874937057,\n",
       " -0.039132826030254364,\n",
       " 0.04414263367652893,\n",
       " -0.004941055551171303,\n",
       " 0.051052697002887726,\n",
       " -0.05431916192173958,\n",
       " 0.011173113249242306,\n",
       " 0.0266990028321743,\n",
       " 0.07509659975767136,\n",
       " 0.048690132796764374,\n",
       " -0.04354275390505791,\n",
       " 0.01333867758512497,\n",
       " -0.10262981057167053,\n",
       " 0.05215669423341751,\n",
       " 0.02223237417638302,\n",
       " -0.029741793870925903,\n",
       " -0.04227066785097122,\n",
       " 0.022984562441706657,\n",
       " 0.03957443684339523,\n",
       " 0.009094778448343277,\n",
       " 0.020864685997366905,\n",
       " 0.00509262178093195,\n",
       " -0.06276409327983856,\n",
       " -0.05021766945719719,\n",
       " -0.005225018598139286,\n",
       " -0.05349820479750633,\n",
       " 0.029773466289043427,\n",
       " 0.021753670647740364,\n",
       " -0.020678048953413963,\n",
       " 0.023866144940257072,\n",
       " 0.005806638393551111,\n",
       " -0.0012137835146859288,\n",
       " 0.022660311311483383,\n",
       " 0.0031567111145704985,\n",
       " 0.030960429459810257,\n",
       " -0.05296808481216431,\n",
       " 0.01867694966495037,\n",
       " -0.1401534080505371,\n",
       " 0.04147820174694061,\n",
       " -0.010278256610035896,\n",
       " -0.011593146249651909,\n",
       " -0.033458251506090164,\n",
       " -0.05058891698718071,\n",
       " 0.04686781391501427,\n",
       " 0.024715127423405647,\n",
       " 0.03349681571125984,\n",
       " 0.11170760542154312,\n",
       " -0.0403486005961895,\n",
       " -0.0042844959534704685,\n",
       " -0.08074361085891724,\n",
       " -0.05611216649413109,\n",
       " 0.03833862021565437,\n",
       " 0.011508360505104065,\n",
       " 0.06873797625303268,\n",
       " -0.03811735659837723,\n",
       " -0.04598833993077278,\n",
       " -0.016439592465758324,\n",
       " 0.024412987753748894,\n",
       " 0.011722046881914139,\n",
       " 0.008168509230017662,\n",
       " 0.039036914706230164,\n",
       " 0.026186540722846985,\n",
       " 0.01048184558749199,\n",
       " 0.04286017641425133,\n",
       " -0.04638158529996872,\n",
       " 0.006481093820184469,\n",
       " 0.044423412531614304,\n",
       " -0.01883763261139393,\n",
       " 0.00726337032392621,\n",
       " 0.05617952719330788,\n",
       " 0.053477976471185684,\n",
       " -0.02148376777768135,\n",
       " -0.084485724568367,\n",
       " -0.012813972309231758,\n",
       " -0.039475537836551666,\n",
       " -0.05802653357386589,\n",
       " 0.03133402392268181,\n",
       " 0.045417316257953644,\n",
       " 0.011839081533253193,\n",
       " -0.017906315624713898,\n",
       " 4.586215086962156e-33,\n",
       " 0.13150791823863983,\n",
       " 0.07932813465595245,\n",
       " -0.09495951235294342,\n",
       " -0.024472175166010857,\n",
       " -0.055816203355789185,\n",
       " -0.009145256131887436,\n",
       " -0.032090410590171814,\n",
       " 0.11379317939281464,\n",
       " -0.14441627264022827,\n",
       " 0.00850959587842226,\n",
       " 0.030667033046483994,\n",
       " -0.01241487916558981,\n",
       " 0.07017052918672562,\n",
       " 0.028179822489619255,\n",
       " 0.04082208126783371,\n",
       " 0.019590837880969048,\n",
       " 0.14295044541358948,\n",
       " 0.0568699836730957,\n",
       " -0.040122076869010925,\n",
       " -0.017843913286924362,\n",
       " -0.06138443946838379,\n",
       " 0.0008760516648180783,\n",
       " -0.054906200617551804,\n",
       " -0.007102516014128923,\n",
       " -0.0004941486986353993,\n",
       " -0.013119390234351158,\n",
       " -0.0022669434547424316,\n",
       " 0.05856667086482048,\n",
       " -0.09971513599157333,\n",
       " -0.025532379746437073,\n",
       " 0.07856457680463791,\n",
       " 0.020517690107226372,\n",
       " -0.004633414093405008,\n",
       " 0.030302615836262703,\n",
       " 0.016833249479532242,\n",
       " 0.09143757820129395,\n",
       " 0.016769731417298317,\n",
       " -0.07980289310216904,\n",
       " 0.04251027852296829,\n",
       " -0.08439735323190689,\n",
       " -0.0238590519875288,\n",
       " 0.04701947048306465,\n",
       " 0.0024148565717041492,\n",
       " 0.10931778699159622,\n",
       " -0.03390537202358246,\n",
       " -0.06402090191841125,\n",
       " -0.037723854184150696,\n",
       " 0.029074762016534805,\n",
       " -0.04244937375187874,\n",
       " 0.015861935913562775,\n",
       " -0.09060660004615784,\n",
       " -0.05576631799340248,\n",
       " 0.022608749568462372,\n",
       " 0.008000322617590427,\n",
       " -0.02246999368071556,\n",
       " 0.022058147937059402,\n",
       " -0.025199150666594505,\n",
       " 0.03033953718841076,\n",
       " 0.010110795497894287,\n",
       " -0.018547305837273598,\n",
       " 0.016989590600132942,\n",
       " 0.0763101875782013,\n",
       " 0.0416206493973732,\n",
       " 0.08752065151929855,\n",
       " -0.012092760764062405,\n",
       " 0.03113519214093685,\n",
       " -0.03219042718410492,\n",
       " 0.012797970324754715,\n",
       " 0.013538764789700508,\n",
       " -0.02975240908563137,\n",
       " 0.03678983077406883,\n",
       " -0.005846079438924789,\n",
       " -0.01105553936213255,\n",
       " 0.03867030143737793,\n",
       " -0.02074146457016468,\n",
       " -0.011265086010098457,\n",
       " -0.023767532780766487,\n",
       " -0.009984560310840607,\n",
       " -0.023108458146452904,\n",
       " 0.012011353857815266,\n",
       " -0.010644523426890373,\n",
       " 0.0513606034219265,\n",
       " -0.02773405611515045,\n",
       " -0.00021139727323316038,\n",
       " 0.0009463390451855958,\n",
       " -0.03158264234662056,\n",
       " 0.051229290664196014,\n",
       " 0.04432860389351845,\n",
       " -0.003773423843085766,\n",
       " -0.04166809841990471,\n",
       " 0.02895456925034523,\n",
       " 0.03306133672595024,\n",
       " -0.015184277668595314,\n",
       " -0.00015773120685480535,\n",
       " -0.04412437602877617,\n",
       " -1.480350064753111e-08,\n",
       " -0.00869594607502222,\n",
       " 0.00013009380199946463,\n",
       " 0.01647108979523182,\n",
       " 0.05922885611653328,\n",
       " 0.04553878307342529,\n",
       " 0.03312685713171959,\n",
       " -0.09335850179195404,\n",
       " -0.03902722895145416,\n",
       " -0.020656151697039604,\n",
       " 0.012607559561729431,\n",
       " 0.06953950226306915,\n",
       " 0.07919187843799591,\n",
       " -0.07191091775894165,\n",
       " -0.0047850944101810455,\n",
       " 0.08800476789474487,\n",
       " 0.04758848249912262,\n",
       " -0.05219274014234543,\n",
       " -0.00752552505582571,\n",
       " -0.05771181359887123,\n",
       " -0.09295844286680222,\n",
       " -0.0045015024952590466,\n",
       " 0.001178039121441543,\n",
       " 0.02444724552333355,\n",
       " -0.06403965502977371,\n",
       " -0.0032275081612169743,\n",
       " -0.02796536311507225,\n",
       " -0.035407572984695435,\n",
       " 0.025036673992872238,\n",
       " -0.009852757677435875,\n",
       " 0.01325240544974804,\n",
       " 0.0011385205434635282,\n",
       " 0.17805425822734833,\n",
       " -0.036146294325590134,\n",
       " -0.007625906262546778,\n",
       " -0.032201241701841354,\n",
       " -0.042293235659599304,\n",
       " 0.004774454515427351,\n",
       " 0.02853180468082428,\n",
       " 0.07472298294305801,\n",
       " -0.014894150197505951,\n",
       " -0.056220654398202896,\n",
       " 0.02723579667508602,\n",
       " -0.011199447326362133,\n",
       " -0.1016678735613823,\n",
       " -0.019529249519109726,\n",
       " 0.027246462181210518,\n",
       " 0.03508107364177704,\n",
       " -0.0816056951880455,\n",
       " -0.0013377582654356956,\n",
       " -0.07635194808244705,\n",
       " -0.03995734825730324,\n",
       " 0.04078188166022301,\n",
       " 0.060128677636384964,\n",
       " 0.07254577428102493,\n",
       " 0.06967505812644958,\n",
       " 0.08909131586551666,\n",
       " 0.015957804396748543,\n",
       " -0.01487354189157486,\n",
       " -0.04674159735441208,\n",
       " -0.013411267660558224,\n",
       " 0.06513479351997375,\n",
       " 0.050905924290418625,\n",
       " 0.051483552902936935,\n",
       " 0.007092184852808714]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_obj = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embed_obj.embed_query(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d2f1856-8fb2-413d-bee4-904a50f2ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader \n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f746de5-5e64-4605-b34b-0614e0b3c414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='28c8b419-8184-4410-a4ae-125e81703ee2', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:'), Document(id='06a6ebf2-6236-4681-ae58-53612a2e36ed', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\")]\n"
     ]
    }
   ],
   "source": [
    "# Load documents\n",
    "loader = TextLoader('my_docs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# split data to chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Generate embedding object\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# stores to vector\n",
    "vector_store = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "# similarity_search\n",
    "response = vector_store.similarity_search('what is langchain?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3aeed-9c6a-455c-ac72-02d28a728482",
   "metadata": {},
   "outputs": [],
   "source": [
    "RecursiveCharacterTextSplitter \n",
    " |\n",
    " |--> paragraph \\n\\n\n",
    "      line \\n\n",
    "       . (sentence)\n",
    "       (word)\n",
    "       '' character "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a7dc6-e43f-40ac-8c45-ae5b97c31dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Groq\n",
    "=====\n",
    "LPU = interfence engine - faster \n",
    "\n",
    "\n",
    "! pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "052b4ff8-b986-4dfb-bd28-5b71b7a9d42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-groq\n",
      "Version: 0.3.6\n",
      "Summary: An integration package connecting Groq and LangChain\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\ProgramData\\anaconda3\\Lib\\site-packages\n",
      "Requires: groq, langchain-core\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ca02a-cd99-4e6f-afc1-25f39df61c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create api key from https://console.groq.com/keys\n",
    "|\n",
    "After getting API key\n",
    "|\n",
    "|->edit .env file\n",
    "    |\n",
    "    GROQ_API_KEY=\"<paste>\"\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c36dc0c3-104e-46d4-a2a1-66fc6356af7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain is an open-source artificial intelligence (AI) framework that enables developers to build custom, scalable, and flexible large language models (LLMs) and applications on top of them. It was founded by David Kan, a well-known figure in the AI and tech community.\\n\\nLangChain is built on top of popular deep learning frameworks like TensorFlow, PyTorch, and Hugging Face Transformers. It provides a set of tools and libraries that make it easier to train, fine-tune, and deploy LLMs, as well as integrate them with other AI and machine learning (ML) models.\\n\\nThe key features of LangChain include:\\n\\n1. **Modular architecture**: LangChain allows developers to break down complex AI models into smaller, reusable modules that can be easily combined and customized.\\n2. **Language model integration**: LangChain provides seamless integration with popular LLMs like BERT, RoBERTa, and others, making it easy to use their capabilities in custom applications.\\n3. **Data processing and management**: LangChain offers tools for data loading, processing, and management, making it easier to work with large datasets.\\n4. **Model deployment and management**: LangChain provides features for deploying and managing LLMs in production environments, including support for Docker containers and Kubernetes.\\n5. **API and SDK**: LangChain comes with a RESTful API and SDK for various programming languages, making it easy to integrate with other systems and applications.\\n\\nLangChain is designed to help developers build more robust, scalable, and maintainable AI applications, and it has gained popularity in the tech community for its flexibility and customizability.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 328, 'prompt_tokens': 40, 'total_tokens': 368, 'completion_time': 0.551926178, 'prompt_time': 0.002341434, 'queue_time': 0.050455846, 'total_time': 0.554267612}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--1bb8cc87-eb42-43ad-8ed2-1e4f974ba4bd-0' usage_metadata={'input_tokens': 40, 'output_tokens': 328, 'total_tokens': 368}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "r = llm.invoke(\"what is langchain?\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6fcdd3-af5b-4144-8370-befe718fd385",
   "metadata": {},
   "outputs": [],
   "source": [
    "Groq does not provide embedding models \n",
    "To use embedding models - HuggingFaceembedding , OpenAI emmbedding, OllamaEmbedding(local m/c)\n",
    "\n",
    "HuggingFaceembedding\n",
    "|\n",
    "sentence-transformers/all-MiniLM-L6-v2 \n",
    "..\n",
    "Groq only for LLM responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3efd8c16-eb85-44b5-a5d5-9b4f63136eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Delhi is the capital city of India. It is situated in the northern part of the country, on the banks of the Yamuna River.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 39, 'total_tokens': 69, 'completion_time': 0.034894916, 'prompt_time': 0.001883963, 'queue_time': 0.054378487, 'total_time': 0.036778879}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--9fa8545a-3fb8-47e2-af79-67087f896709-0' usage_metadata={'input_tokens': 39, 'output_tokens': 30, 'total_tokens': 69}\n"
     ]
    }
   ],
   "source": [
    "r = llm.invoke(\"Where is Delhi?\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ae943-dbd1-40b1-844c-04c4238b6335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Load the data (ex: TextLoader().load())\n",
    "|\n",
    "Chunk - splitted.split_documents() \n",
    "|\n",
    "Embedding_obj\n",
    "|\n",
    "Vector_Store\n",
    "\n",
    "vector_store.similarity_search('Query') ----> .....\n",
    "---\n",
    "\n",
    "Retriver \n",
    "|\n",
    "llm\n",
    "|\n",
    "Retriever_Chain\n",
    "|\n",
    "prompt\n",
    "|\n",
    "Prompt|Retriever_Chain --->invoke(Query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefe9c3f-2059-46fb-bbf0-79bb8b90f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Retriever - fetches the most relevent documents from database(vectorstore) based on the query\n",
    "\n",
    "# vector_store.similarity_search('Query') ----> .....\n",
    "vector_store.as_retiever() ->retriver_obj\n",
    "\n",
    "A retiever is the component of RAG\n",
    "\n",
    "1. Takes a user query - Convert user query into a vector (Embeds the query)\n",
    "2. Finds the most relevent chunks from vectorDB - Finds top - n similar vectors- (Searches the DB)\n",
    "3. Returns those chunks back to llm - sends them to LLM (Returns documents) \n",
    "\n",
    "4. now LLM sees only filterd,relevent chunks (not whole database content)\n",
    "\n",
    "db.as_reriever()   -> retriverobject\n",
    "    |\n",
    "  Chroma.from_texts(docs,embeddings) ->db\n",
    "    |\n",
    " retriverobject.get_revelent_documents('user_query')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce61abdd-43b3-4c84-8a92-55424dbef27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader \n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8df6e392-939a-4029-8e4f-b5b162fb4aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_13604\\2698380472.py:29: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  qa_chain(\"what is langchain?\")\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is langchain?',\n",
       " 'result': 'LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies the entire LLM application lifecycle, making it easier to build, deploy, and maintain applications with LLMs.'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step-1 load the data\n",
    "loader = TextLoader('my_docs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "#Step-2 split - chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "#Step-3 embeddingobject\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "#Step-4 stores to vectorDB\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "#Step-5 similarity_search ==> retriver_object\n",
    "#vectorstore.similarity_search(\"what is langchain?\")\n",
    "retriever_obj = vectorstore.as_retriever() # Convert vector stores into a retriver object\n",
    "\n",
    "#Step-6: create llm objct\n",
    "llm_obj = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "#Step-7: Build Retrieval QA Chain -->QA_Chain_object\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj)\n",
    "\n",
    "#Step-8: QA_Chain_object('Query')\n",
    "qa_chain(\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02e4efcd-89e2-493d-bc32-0206627ba7da",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-core\n",
      "Version: 0.3.79\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\ProgramData\\anaconda3\\Lib\\site-packages\n",
      "Requires: jsonpatch, langsmith, packaging, pydantic, PyYAML, tenacity, typing-extensions\n",
      "Required-by: langchain, langchain-chroma, langchain-community, langchain-groq, langchain-huggingface, langchain-ollama, langchain-openai, langchain-text-splitters, langgraph, langgraph-checkpoint, langgraph-prebuilt\n"
     ]
    }
   ],
   "source": [
    "! pip show langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f985c6e8-979a-4330-a8d4-9e872e3c999d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-community\n",
      "Version: 0.3.27\n",
      "Summary: Community contributed LangChain integrations.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\n",
      "Requires: aiohttp, dataclasses-json, httpx-sse, langchain, langchain-core, langsmith, numpy, pydantic-settings, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show langchain-community"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c215fcb-5c33-4626-b26f-a8b1aadecb12",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7007a74-4ce2-445d-b5cc-32b60365a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import RetrievalQA\n",
    "# this will work in higher version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb151f47-b336-4571-8fde-ba890f747813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.27'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90df4a3-27da-4435-a3df-2287e1b58af1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "upgraded first..\n",
    "!pip install --upgrade langchain langchain-community langchain-core\n",
    "\n",
    "then used \n",
    "! pip install langchain.chains\n",
    "or\n",
    "! pip install langchain-chains\n",
    "neither of this worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abefd413-6dc0-42ca-a712-64e2ea0bd3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader \n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1235edb9-1acd-43e4-a456-aa35ee4a5d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is langchain?',\n",
       " 'result': 'LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies every stage of the LLM application lifecycle, from development to productionization and deployment.'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step-1 load the data\n",
    "loader = TextLoader('my_docs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "#Step-2 split - chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "#Step-3 embeddingobject\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "#Step-4 stores to vectorDB\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "#Step-5 similarity_search ==> retriver_object\n",
    "#vectorstore.similarity_search(\"what is langchain?\")\n",
    "retriever_obj = vectorstore.as_retriever() # Convert vector stores into a retriver object\n",
    "\n",
    "#Step-6: create llm objct\n",
    "llm_obj = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "#Step-7: Build Retrieval QA Chain -->QA_Chain_object\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj)\n",
    "\n",
    "#Step-8: QA_Chain_object('Query')\n",
    "qa_chain(\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "498ddc1f-b346-40ac-bffa-3aafcd42faf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'How to write factorial program in C language?',\n",
       " 'result': \"I don't have information on how to do that. The question seems to be about writing a program in C, but I only have knowledge about LangChain, which is a framework for developing applications powered by large language models (LLMs).\"}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain(\"How to write factorial program in C language?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6965e9e3-67ac-4a24-9bea-60aaa161321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is langchain and how RAG is working?',\n",
       " 'result': \"**What is LangChain?**\\n\\nLangChain is an open-source framework for developing applications powered by large language models (LLMs). It simplifies the entire LLM application lifecycle, making it easier to build, deploy, and maintain applications that leverage the power of LLMs. LangChain provides a set of open-source components and third-party integrations to help developers create a wide range of applications, from conversational AI to content generation and more.\\n\\n**LangChain Components:**\\n\\n1. **LangGraph**: A graph-based data structure that allows developers to build stateful agents with first-class streaming and human-in-the-loop support. LangGraph enables the creation of complex, dynamic models that can interact with users and other systems in a more natural way.\\n2. **LangSmith**: A set of tools for inspecting, monitoring, and evaluating LangChain applications. LangSmith helps developers optimize and deploy their applications with confidence.\\n3. **LangGraph Platform**: A platform for turning LangGraph applications into production-ready APIs and Assistants.\\n\\n**How does RAG (Retriever-Generator Architecture) work in LangChain?**\\n\\nRAG is a popular architecture for building applications with LLMs. It consists of two main components:\\n\\n1. **Retriever**: A component that retrieves relevant information from a large dataset or knowledge base. In LangChain, this can be implemented using a LangGraph agent that indexes and queries a dataset.\\n2. **Generator**: A component that generates text based on the information retrieved by the retriever. In LangChain, this can be implemented using a large language model (LLM) such as a transformer-based model.\\n\\nIn LangChain, a RAG architecture can be built using the following components:\\n\\n1. **LangGraph**: Create a retriever agent that indexes and queries a dataset.\\n2. **LLM**: Use a large language model (LLM) as a generator to produce text based on the information retrieved by the retriever.\\n\\nWhen a user interacts with the application, the retriever agent retrieves relevant information from the dataset, and the generator LLM produces text based on that information. LangChain's LangGraph platform can then be used to deploy the application as a production-ready API or Assistant.\\n\\nI hope this helps! Let me know if you have any further questions.\"}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain(\"what is langchain and how RAG is working?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b6ab089-6780-4ab5-a7fe-6c1677355071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is langchain and how to write factorial program?',\n",
       " 'result': 'LangChain is an open-source framework for developing applications powered by large language models (LLMs). It simplifies every stage of the LLM application lifecycle, from development to productionization and deployment.\\n\\nTo write a factorial program using LangChain, we\\'ll use the LangGraph component, which allows us to build stateful agents with first-class streaming and human-in-the-loop support. Here\\'s an example code snippet:\\n\\n```python\\nimport langchain\\nfrom langchain.chains import llama\\nfrom langchain.llms import llama as llma\\n\\n# Initialize the LLM\\nllm = llma.LLaMA()\\n\\n# Define the function to calculate the factorial\\ndef calculate_factorial(num):\\n    return int(llm.call(f\"calculate the factorial of {num}\"))\\n\\n\\n# Test the function\\nprint(calculate_factorial(5))  # Output: 120\\n\\n```\\n\\nIn this code snippet, we first import the necessary LangChain libraries and initialize the LLM. We then define a function `calculate_factorial` that takes an integer input `num` and uses the LLM to calculate the factorial. The `llm.call` method sends a prompt to the LLM and returns the response. We test the function by calling it with the input `5`.\\n\\nPlease note that you need to have LangChain installed in your environment to run this code. You can install it using pip:\\n\\n```bash\\npip install langchain\\n```'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain(\"what is langchain and how to write factorial program?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f72955d2-96d0-474b-8f87-897af54f1b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**Factorial Program in C**\\n==========================\\n\\nThe factorial of a non-negative integer `n`, denoted by `n!`, is the product of all positive integers less than or equal to `n`.\\n\\n### Code\\n\\n```c\\n#include <stdio.h>\\n\\n// Function to calculate factorial\\nlong long factorial(int n) {\\n    if (n == 0 || n == 1) {\\n        return 1;\\n    } else {\\n        return n * factorial(n - 1);\\n    }\\n}\\n\\nint main() {\\n    int num;\\n    printf(\"Enter a non-negative integer: \");\\n    scanf(\"%d\", &num);\\n\\n    if (num < 0) {\\n        printf(\"Error: Factorial is not defined for negative numbers.\");\\n    } else {\\n        long long result = factorial(num);\\n        printf(\"Factorial of %d = %lld\\\\n\", num, result);\\n    }\\n\\n    return 0;\\n}\\n```\\n\\n### Explanation\\n\\n1.  We define a function `factorial` that takes an integer `n` as input and returns its factorial.\\n2.  The function uses recursion to calculate the factorial. If `n` is 0 or 1, it returns 1, since the factorial of 0 and 1 is 1.\\n3.  For `n > 1`, the function calls itself with `n - 1` and multiplies the result by `n`.\\n4.  In the `main` function, we prompt the user to enter a non-negative integer.\\n5.  We validate the input to ensure it is non-negative. If the input is negative, we print an error message.\\n6.  If the input is non-negative, we call the `factorial` function with the input value and store the result in the `result` variable.\\n7.  Finally, we print the result to the console.\\n\\n### Notes\\n\\n*   We use `long long` to store the factorial result, as it can be a large number.\\n*   We use recursion, but keep in mind that it may lead to a stack overflow for large inputs.\\n*   For very large inputs, consider using an iterative approach or a specialized library for large number arithmetic.\\n\\n### Example Use Cases\\n\\n*   Input: `5`\\n    Output: `Factorial of 5 = 120`\\n*   Input: `0`\\n    Output: `Factorial of 0 = 1`\\n*   Input: `-3`\\n    Output: `Error: Factorial is not defined for negative numbers.`', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 518, 'prompt_tokens': 43, 'total_tokens': 561, 'completion_time': 0.598270791, 'prompt_time': 0.003066401, 'queue_time': 0.056484879, 'total_time': 0.601337192}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--8656ba34-ae40-4535-aa90-893c64fccf66-0', usage_metadata={'input_tokens': 43, 'output_tokens': 518, 'total_tokens': 561})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_obj.invoke(\"How to write factorial program in C?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a40040-f6d1-4b75-a9f3-131eefbc8cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8adbd913-5242-4c82-8e45-c6951b964291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm_obj = OllamaLLM(model='gemma2:2b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4336a21c-ebd3-4286-95de-7b0554c9d060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is langchain?',\n",
       " 'result': 'LangChain is a framework for developing applications powered by large language models (LLMs). \\n'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj)\n",
    "\n",
    "#Step-8: QA_Chain_object('Query')\n",
    "qa_chain(\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "96520c27-085a-42e1-9180-a4ff3634da4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'How to write factorial program in C?',\n",
       " 'result': \"I don't know how to write a factorial program in C. \\n\"}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain(\"How to write factorial program in C?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30f3ea77-77de-4400-bf24-4796b3fa0947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'How to write factorial program in C?', 'result': \"I don't know. \\n\"}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain(\"How to write factorial program in C?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87997d2d-c648-4997-a368-7715aef61d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'How to write factorial program in C?',\n",
       " 'result': 'This document focuses on using LangChain, a framework for building applications with large language models, and does not provide code examples or information about calculating factorials.  \\n \\n Therefore, I cannot answer how to write a factorial program in C from this context. \\n'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain(\"How to write factorial program in C?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c774384c-61bb-4734-8490-5f95ccc46e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=False, combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=OllamaLLM(model='gemma2:2b'), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000287542DF890>, search_kwargs={}))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ce9d344b-ddae-46b8-8efe-e08e41d70404",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "46c036ce-8df5-473f-ad51-526f323d117c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'How to write factorial program in C?',\n",
       " 'result': \"I don't know how to answer that question. \\n\"}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain(\"How to write factorial program in C?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b28e50-540e-4eb0-bee4-187d3f66da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82867497-f9bb-43dc-ac75-3ad337d9f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt - text \n",
    "jinja2 template code => {{variable}} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "be0d1d3a-f772-418f-8223-0af631524ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I likes to read python book')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "PromptTemplate.from_template(\"I likes to read python book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1cccb4f6-8931-431d-81b2-382f85c697ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I likes to read java book')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template(\"I likes to read java book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a497b0ad-3121-4822-a189-11aa45936eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read java book'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = PromptTemplate.from_template(\"I likes to read {myvar} book\")\n",
    "obj.format(myvar=\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0a53cb2f-d268-45d0-be46-07080b10ec96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read story book'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.format(myvar=\"story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e0ca0425-b3f8-487f-bac0-52fab83d03dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['myvar'], input_types={}, partial_variables={}, template='I likes to read {myvar} book')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "298622da-ae5c-4c0b-aa8c-f9afba27b294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['author', 'my_year', 'myvar'], input_types={}, partial_variables={}, template='I likes to read {myvar} book written by {author} released on {my_year}')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = PromptTemplate.from_template('I likes to read {myvar} book written by {author} released on {my_year}')\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d893025e-402b-4b65-bac3-aadf9833ed40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read birds book written by Mr.abc released on 2004'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.format(myvar=\"birds\",author=\"Mr.abc\",my_year=2004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1954627e-e133-40f4-9ff7-49ae7eaf5ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatPromptTemplate \n",
    "   |-> Structure conversation\n",
    "          |\n",
    "        End_user -> Human Message\n",
    "        LLM response --> AI Message\n",
    "         |\n",
    "        system - instruction \n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "58cad17d-8604-4b56-b535-48a3eccecb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c8535a0d-bbb1-4df5-a140-a26a443764a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ChatPromptTemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6bf9a00c-1131-4caf-b28e-46ef20ff3718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['myvar'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are helpful assistant'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['myvar'], input_types={}, partial_variables={}, template='I likes to read {myvar} books'), additional_kwargs={})])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','you are helpful assistant'),\n",
    "        ('human','I likes to read {myvar} books')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d0161fc8-7351-4a0b-8088-f990161524d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','you are helpful assistant'),\n",
    "        ('human','I likes to read {myvar} books')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "39543ece-5f07-416c-903f-a5c89628a5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I likes to read science books', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format_messages(myvar=\"science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c844543-2856-4dac-832e-74dec3fe6a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.invoke(\"what is langchain?\") ===> HumanMessage(content='what is langchain?', additional_kwargs={}, response_metadata={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e8e36fde-6c5e-4130-b4b1-c58a64264885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I likes to read science books', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Sure', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','you are helpful assistant'),\n",
    "        ('human','I likes to read {myvar} books'),\n",
    "        ('ai',\"Sure\")\n",
    "    ]\n",
    ")\n",
    "prompt.format_messages(myvar=\"science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e3bce25f-ad88-4695-be53-569c0a4b31d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_llm_obj = OllamaLLM(model=\"gemma2:2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dc01dfe3-4db4-414c-a9e2-6a632a6d3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"don't Know about it\"),\n",
    "    (\"user\",\"Question:{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0d80c016-001a-4972-b2bf-f801e460a710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"don't know\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Question:{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e7b35119-dd86-48d1-8fbf-b487eca9c8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It seems like you are asking about \"LangChain\"! \\n\\n**LangChain** is a powerful framework designed to make building applications with large language models (LLMs) much easier and more efficient. Think of it as a set of tools that helps you connect LLMs to real-world data, allowing for more sophisticated tasks than just simply generating text.\\n\\nHere\\'s a breakdown:\\n\\n* **Connects LLMs to external data:** Instead of just using LLMs in isolation, LangChain lets them interact with databases, APIs, and other sources of information, making them much smarter and adaptable. \\n* **Creates reusable components:**  It provides pre-built modules (like chains) for common tasks like summarization, question answering, or text generation. This allows developers to build complex applications faster and more effectively.\\n* **Flexible & customizable:** LangChain emphasizes flexibility. You can customize its components and even create your own custom tools tailored to specific needs.\\n\\n\\n**In simpler terms:** LangChain makes it easier to:\\n\\n* Use LLMs in real-world applications (like chatbots, question-answering systems, etc.) \\n* Build more complex LLM-based applications.\\n* Focus on the core logic of your application instead of getting bogged down by implementation details.\\n\\n\\n**To learn more about LangChain:**  Check out their official website or read their documentation for detailed information and examples! [https://langchain.readthedocs.io/en/latest/](https://langchain.readthedocs.io/en/latest/) \\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain  = chat_prompt|my_llm_obj\n",
    "chain.invoke({'question':'what is langchain?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ce32f9b3-1f09-4890-8cc5-ebbf9ff8e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([ \n",
    "    (\"system\",\"Say don't know\"),(\"user\",\"Question:{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "efc3fe86-9975-4415-b650-62ac9392f3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_13604\\275367955.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  my_llm_obj = Ollama(model=\"gemma2:2b\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "my_llm_obj = Ollama(model=\"gemma2:2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "738fff6f-d1a2-40dd-be66-e5e6e90640e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chain = chat_prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d24cea4f-44dd-4c29-aaaa-09c04db8468c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't know. \\n\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chain.invoke({'question':'what is genAI?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "613346b0-4feb-431f-97b0-721a6177102e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't know. \\n\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chain.invoke({'question':'what is langchain?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cc13ad0a-62ee-4b32-89d8-37215d3d483a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Say: Don't know \\n\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chain.invoke({'question':'How to write Hello world program?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "966707aa-71a9-430a-9a65-03c377e714b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"System: I'm sorry, but I don't understand your question. Can you rephrase it? \\n\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chain.invoke({'question':'adfdsaffsadsf23424asf#@$#@sadfasd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "61844e21-55e3-41b8-a073-c58c948a4874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"System: Don't know \\n\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chain.invoke({'question':'what is python'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8ddb016f-cec0-4ad0-85ef-ce0d30c65833",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([ \n",
    "    (\"system\",\"\"),(\"user\",\"Question:{question}\")\n",
    "])\n",
    "my_chain = chat_prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b8a4c11b-2d4d-47fa-8c5d-d51832453395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python is a popular and versatile programming language known for its clear syntax and readability.  Here\\'s a breakdown of what it is:\\n\\n**What is Python?**\\n\\n* **A High-Level Language:** It\\'s designed to be easier to understand and use than low-level languages that directly interact with a computer\\'s hardware (e.g., assembly language).\\n* **General-Purpose:**  Python can be used for various tasks, including:\\n    * **Web development:** Creating websites, web applications, and APIs (Application Programming Interfaces) \\n    * **Data science and analysis:** Handling large datasets, analyzing trends, and building machine learning models.\\n    * **Scripting and automation:**  Automating repetitive tasks or performing quick coding for tasks like system administration or file manipulation. \\n    * **Game development:** Making simple games using its libraries and modules.\\n    * **Desktop applications:** Creating user-friendly graphical interfaces (GUIs).\\n\\n**Why is Python Popular?**\\n\\n* **Beginner-Friendly:**  Its syntax resembles natural language, making it easy for new programmers to learn. \\n* **Large Community and Resources:** A massive community contributes to vast libraries, frameworks, and support resources.\\n* **Versatile Libraries:** Python offers a wide range of pre-built modules (like NumPy for math, Pandas for data analysis, and TensorFlow for machine learning) that save time and effort in complex projects.\\n* **Cross-Platform:**  It runs on various operating systems (Windows, macOS, Linux) without significant modifications. \\n\\n**Example:**\\n\\n ```python\\n print(\"Hello, world!\")  # Outputs: Hello, world! \\n ```\\n\\nIf you\\'re interested in learning more about Python, there are tons of online resources like websites and tutorials. You can explore:\\n\\n* **Official Python website:** https://www.python.org/\\n* **W3Schools Python Tutorial:** https://www.w3schools.com/python/ \\n\\n\\n\\nLet me know if you have any other questions! \\n'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chain.invoke({'question':'what is python'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd3ffd-7957-4929-bfc0-3abe740aaf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Summarization\n",
    "#  Summary -->Text (or) report - 5 bullet points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2f0304c5-b114-4283-8606-78beb70cdd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Aritfical Intelligence (AI) is transforming industries..\n",
    "AI models are used in hearlhcare,finance,education,enterprise indus and also supports autonomous system'''\n",
    "\n",
    "prompt = f'Summarize the following text in 3 bullet points:\\n\\n{text}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "467d0cd4-7e13-4d14-96cd-fb22e796fe20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a 3-bullet summary of the text:\\n\\n* **AI revolutionizes diverse sectors:** Artificial Intelligence (AI) applications are being integrated into healthcare, finance, education, enterprise operations, and even autonomous systems.\\n* **Healthcare:**  AI is improving patient care through diagnosis assistance, personalized treatment plans, and drug discovery. \\n* **Wide-ranging impact:** AI's influence stretches across various industries, automating tasks, providing insights, and enhancing efficiency across numerous fields. \\n\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_llm_obj.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "06759280-a57c-4005-a0fb-a48c8550ed28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='It seems like there\\'s a typo in the text. I\\'ll assume you meant \"Artificial Intelligence\" (AI). Here are 3 bullet points summarizing the text:\\n\\n Artificial Intelligence (AI) is transforming various industries.\\n AI models are being used in fields like healthcare, finance, education, and enterprise industries.\\n AI also supports the development of autonomous systems.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 80, 'total_tokens': 156, 'completion_time': 0.110797116, 'prompt_time': 0.00443855, 'queue_time': 0.04862568, 'total_time': 0.115235666}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--daf7d27e-e057-47af-9993-5ab0d90e6b5c-0', usage_metadata={'input_tokens': 80, 'output_tokens': 76, 'total_tokens': 156})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_obj = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "llm_obj.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dcdd575a-89f9-49ec-b51b-c32715ade389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There seems to be a typo in the provided text. I'll assume it's \"Artificial Intelligence\" instead of \"Aritfical Intelligence\". Here are 3 bullet points summarizing the text:\n",
      "\n",
      "* Artificial Intelligence (AI) is transforming various industries.\n",
      "* AI models are being applied in sectors such as healthcare, finance, education, and enterprise.\n",
      "* AI also supports the development of autonomous systems.\n"
     ]
    }
   ],
   "source": [
    "response = llm_obj.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e894083f-8cca-4813-80eb-51291f178cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = open('my_docs.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5d56763b-a21b-4cbd-a024-6030df364164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 bullet points summarizing the text:\n",
      "\n",
      "- LangChain is a framework for developing applications powered by large language models (LLMs).\n",
      "- The framework simplifies the development process through open-source components and third-party integrations.\n",
      "- LangChain offers tools for building stateful agents, including LangGraph, which supports streaming and human-in-the-loop interactions.\n",
      "- LangSmith is a tool for inspecting, monitoring, and evaluating applications to ensure optimal performance and deployment.\n",
      "- The framework provides a platform, LangGraph Platform, for turning applications into production-ready APIs and Assistants.\n"
     ]
    }
   ],
   "source": [
    "prompt = f'Summarize the following text in 5 bullet points:\\n\\n{s}'\n",
    "response = llm_obj.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d796285-521a-42b8-96ae-292b59e4ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization\n",
    "\n",
    "# Grammar and Text Corrections\n",
    "\n",
    "# Text to Structured ( json/yaml/dataframe)\n",
    "\n",
    "# Code generation \n",
    "\n",
    "# Content Generation - Rewrite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3fc186ac-5391-429c-99c4-36458a3faee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here\\'s the corrected text:\\n\\n\"AI is changing the world in many ways. It helps people to work faster.\"\\n\\nCorrected mistakes:\\n\\n- \"AI are\" -> \"AI is\" (subject-verb agreement)\\n- \"chaining\" -> \"changing\" (correct verb)\\n- \"way\" -> \"ways\" (plural noun)\\n- \"peoples\" -> \"people\" (correct spelling)\\n- \"It help\" -> \"It helps\" (verb agreement)' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 62, 'total_tokens': 158, 'completion_time': 0.13460698, 'prompt_time': 0.004776465, 'queue_time': 0.051749035, 'total_time': 0.139383445}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--ed56742f-f42d-4de2-bed3-7110686f5305-0' usage_metadata={'input_tokens': 62, 'output_tokens': 96, 'total_tokens': 158}\n"
     ]
    }
   ],
   "source": [
    "# Grammar and Text Corrections\n",
    "prompt = '''\n",
    "Correct grammar and spelling misktakes in this text:\n",
    "\"AI are chaining the world in many way.It help peoples to work faster\"\n",
    "'''\n",
    "response = llm_obj.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "af0e0a7f-a40c-4481-afba-40c0583f3456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the corrected text:\n",
      "\n",
      "\"AI is changing the world in many ways. It helps people to work faster.\"\n",
      "\n",
      "Corrected mistakes:\n",
      "\n",
      "- \"AI are\" -> \"AI is\" (subject-verb agreement)\n",
      "- \"chaining\" -> \"changing\" (correct verb)\n",
      "- \"way\" -> \"ways\" (plural noun)\n",
      "- \"peoples\" -> \"people\" (correct spelling)\n",
      "- \"It help\" -> \"It helps\" (verb agreement)\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "234c8b77-698b-4894-b3a3-51aa5caa55e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='To extract the information in JSON format, I\\'ll use the given text. \\n\\nHere\\'s the extracted information in JSON format:\\n\\n```json\\n{\\n  \"company\": \"Ctpl Indus\",\\n  \"founder\": \"Mr. Ram\",\\n  \"year\": 1982,\\n  \"location\": \"Bangalore\"\\n}\\n```\\n\\nHowever, since you provided just a text snippet, I\\'ll assume the text is about Ctpl Indus.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 69, 'total_tokens': 160, 'completion_time': 0.149397335, 'prompt_time': 0.003978492, 'queue_time': 0.049844078, 'total_time': 0.153375827}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--ee861af5-d5c9-47e5-aa5d-bf054aaae947-0' usage_metadata={'input_tokens': 69, 'output_tokens': 91, 'total_tokens': 160}\n",
      "\n",
      "To extract the information in JSON format, I'll use the given text. \n",
      "\n",
      "Here's the extracted information in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"company\": \"Ctpl Indus\",\n",
      "  \"founder\": \"Mr. Ram\",\n",
      "  \"year\": 1982,\n",
      "  \"location\": \"Bangalore\"\n",
      "}\n",
      "```\n",
      "\n",
      "However, since you provided just a text snippet, I'll assume the text is about Ctpl Indus.\n"
     ]
    }
   ],
   "source": [
    "# Text to structured format (ex: Json)\n",
    "text = \"Ctpl Indus was founded by Mr.Ram in year 1982 in Bangalore\"\n",
    "\n",
    "prompt = f'''From the given text:{text} extract information in json: company,founder,year,location'''\n",
    "\n",
    "response = llm_obj.invoke(prompt)\n",
    "print(response)\n",
    "print('')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ac9c7da0-b259-4716-8125-f04e5c7bbba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Calculating Factorial of a Number in Python**\n",
      "=====================================================\n",
      "\n",
      "Here's a simple Python function to calculate the factorial of a given number:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    \"\"\"\n",
      "    Calculates the factorial of a given integer.\n",
      "\n",
      "    Args:\n",
      "        n (int): The number for which to calculate the factorial.\n",
      "\n",
      "    Returns:\n",
      "        int: The factorial of n.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If n is a negative integer.\n",
      "    \"\"\"\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"Input must be an integer.\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Input must be a non-negative integer.\")\n",
      "    elif n == 0 or n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        result = 1\n",
      "        for i in range(2, n + 1):\n",
      "            result *= i\n",
      "        return result\n",
      "```\n",
      "\n",
      "**Example Use Cases**\n",
      "-----------------------\n",
      "\n",
      "```python\n",
      "print(factorial(5))  # Output: 120\n",
      "print(factorial(3))  # Output: 6\n",
      "print(factorial(0))  # Output: 1\n",
      "print(factorial(1))  # Output: 1\n",
      "\n",
      "# Test error handling\n",
      "try:\n",
      "    print(factorial(-5))\n",
      "except ValueError as e:\n",
      "    print(e)  # Output: Input must be a non-negative integer.\n",
      "\n",
      "try:\n",
      "    print(factorial(3.5))\n",
      "except TypeError as e:\n",
      "    print(e)  # Output: Input must be an integer.\n",
      "```\n",
      "\n",
      "This function uses a simple iterative approach to calculate the factorial, which is efficient for small to medium-sized inputs. For larger inputs, a recursive approach or a more advanced algorithm might be necessary to avoid stack overflows or performance issues.\n"
     ]
    }
   ],
   "source": [
    "# Code Generation\n",
    "# ---------------\n",
    "\n",
    "prompt=\"Write a python function to calculate factorial of a number\"\n",
    "\n",
    "response = llm_obj.invoke(prompt)\n",
    "print(response.content)\n",
    "\n",
    "# print(llm_obj.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e6a913dd-e6ef-4e66-bd59-81dd95f8d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue because of a phenomenon called Rayleigh scattering. This is the scattering of light by small particles or molecules in the atmosphere.\n",
      "\n",
      "Here's a simplified explanation:\n",
      "\n",
      "1. **Sunlight enters the Earth's atmosphere**: When sunlight enters the atmosphere, it contains all the colors of the visible spectrum, which we see as white light.\n",
      "2. **Light interacts with tiny molecules**: The light encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2) in the atmosphere. These molecules are much smaller than the wavelength of light.\n",
      "3. **Scattering occurs**: When light hits these tiny molecules, it scatters in all directions. This scattering is more pronounced for shorter (blue) wavelengths than for longer (red) wavelengths.\n",
      "4. **Blue light is scattered more**: Because blue light has a shorter wavelength, it is scattered more than other colors by the tiny molecules in the atmosphere.\n",
      "5. **Our eyes see the scattered light**: As we look up at the sky, we see the scattered blue light from all directions, making the sky appear blue.\n",
      "\n",
      "This explanation is an oversimplification, but it captures the main idea behind the sky's blue color.\n"
     ]
    }
   ],
   "source": [
    "Query=\"Why is the sky blue?\"\n",
    "prompt=f'Answer this question clearly:\\n{Query}'\n",
    "\n",
    "print(llm_obj.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9c3d331f-e5a6-49a9-aeee-d38402cdcb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a rewritten version of the given text:\n",
      "\n",
      "\"Our product is affordable and beneficial for every customer, suitable for both formal and professional settings.\"\n"
     ]
    }
   ],
   "source": [
    "# Text -->Rewrite\n",
    "\n",
    "text = \"Our product is afforable and useful for every customers\"\n",
    "prompt = f\"Rewrite the given text:{text} in formal and profession:\"\n",
    "\n",
    "print(llm_obj.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c6da2fc8-db71-43bb-8622-bd93e8008c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an example of a professional appointment email for a selected candidate:\n",
      "\n",
      "Subject: Job Offer Acceptance and Appointment Details - [Position]\n",
      "\n",
      "Dear [Candidate Name],\n",
      "\n",
      "We are pleased to inform you that after a thorough evaluation of your application and interview, we are pleased to offer you the position of [Position] at [Company Name]. We believe your skills and experience make you an excellent fit for our team, and we are excited to have you on board.\n",
      "\n",
      "Below are the details of your appointment:\n",
      "\n",
      "- **Position:** [Position]\n",
      "- **Salary:** [Salary]\n",
      "- **Reporting Manager:** [Manager's Name]\n",
      "- **Expected Start Date:** [Expected Start Date]\n",
      "\n",
      "To confirm your acceptance of this offer, please sign and return one copy of this email to us by [Deadline for Response]. We also require a start date confirmation and any other relevant details as mentioned below.\n",
      "\n",
      "- **Acceptance:** Please confirm your acceptance of this offer by signing and returning this email to us.\n",
      "- **Start Date:** Please confirm your expected start date.\n",
      "- **Benefits and Policies:** We will be sending you a separate email with details of our benefits and policies, including health insurance, retirement plans, and other company policies.\n",
      "\n",
      "If you have any questions or concerns, please do not hesitate to contact us. We are looking forward to welcoming you to our team.\n",
      "\n",
      "Please find the job offer acceptance form attached to this email.\n",
      "\n",
      "Acceptance Form:\n",
      "\n",
      "I, [Candidate Name], accept the offer of employment with [Company Name] for the position of [Position], as outlined in this email. I confirm that I will start on [Expected Start Date] and will be available to work the required hours.\n",
      "\n",
      "Signature: ______________________________\n",
      "Date: ______________________________\n",
      "\n",
      "Please return the signed acceptance form to us by [Deadline for Response] to confirm your acceptance of this offer.\n",
      "\n",
      "Thank you for your interest in [Company Name], and we look forward to having you on board.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]\n",
      "[Your Title]\n",
      "[Company Name]\n",
      "[Contact Information]\n"
     ]
    }
   ],
   "source": [
    "input_var = 'Give me professional appoitment email for a candidate who was selected'\n",
    "print(llm_obj.invoke(input_var).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "37a40526-eb1d-4ccf-ae4c-7ec5165e0f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## continue ChatPrompt Template\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your are helpful AI assistant\"),\n",
    "    (\"user\",\"Question:{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5aa81955-c4ae-4a27-af53-9a37e3d6777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_obj = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f4144442-3d40-41db-a028-9f5362b94188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Your are helpful AI assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Question:What is langchain?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.format_messages(question=\"What is langchain?\") # not connected to llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b8faa6b0-4309-49f2-9800-e4c10967c237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='What is Langchain?', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "HumanMessage(content=\"What is Langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "08f55658-fbe3-4562-8c8d-c7f68affd2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langchain is an open-source artificial intelligence development platform that enables users to build, deploy, and manage custom AI applications. It is designed to simplify the process of integrating multiple AI models and services, allowing developers to create more sophisticated and context-aware AI systems.\\n\\nLangchain provides a suite of tools and APIs for integrating various AI models, including natural language processing (NLP), computer vision, and other specialized models. This enables developers to leverage the strengths of multiple AI models and create more comprehensive and accurate AI applications.\\n\\nSome key features of Langchain include:\\n\\n1. **Model integration**: Langchain allows developers to integrate multiple AI models and services, such as NLP, computer vision, and speech recognition, into a single application.\\n2. **Contextual understanding**: Langchain's architecture is designed to capture and utilize contextual information, enabling AI applications to better understand the nuances of human language and behavior.\\n3. **Customizable**: Langchain's platform is highly customizable, allowing developers to create AI applications tailored to specific use cases and industries.\\n4. **Scalability**: Langchain is designed to scale with the needs of the application, making it suitable for large-scale AI deployments.\\n5. **Open-source**: Langchain is an open-source project, which means that developers can access and contribute to the platform's codebase, fostering a community-driven approach to AI development.\\n\\nLangchain is particularly well-suited for applications that require complex decision-making, such as:\\n\\n1. **Chatbots**: Langchain can be used to build advanced chatbots that can understand and respond to nuanced user queries.\\n2. **Virtual assistants**: Langchain's capabilities can be leveraged to create virtual assistants that can perform a wide range of tasks, from scheduling appointments to making recommendations.\\n3. **Content generation**: Langchain can be used to generate high-quality content, such as articles, social media posts, and product descriptions.\\n4. **Data analysis**: Langchain's AI capabilities can be applied to data analysis tasks, such as identifying patterns and trends in large datasets.\\n\\nOverall, Langchain is a powerful platform for building custom AI applications that can understand and interact with humans in a more sophisticated and contextual way.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 439, 'prompt_tokens': 40, 'total_tokens': 479, 'completion_time': 0.61934457, 'prompt_time': 0.010087242, 'queue_time': 0.056776357, 'total_time': 0.629431812}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c250d7ab-94af-46c8-9b6e-ccb466ec5f41-0', usage_metadata={'input_tokens': 40, 'output_tokens': 439, 'total_tokens': 479})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm_obj.invoke(\"What is Langchain?\")\n",
    "llm_obj.invoke([HumanMessage(content=\"What is Langchain?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9b7c7b5c-8c22-4352-a7b9-481add296ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Karthik sir. I'm happy to meet you. As an instructor, I'm sure you're here to teach or ask something. Please feel free to share your query or topic you'd like to discuss, and I'll do my best to assist you.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 45, 'total_tokens': 100, 'completion_time': 0.107509829, 'prompt_time': 0.002188291, 'queue_time': 0.049169528, 'total_time': 0.10969812}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2bb84edd-e63a-4db8-abee-a0dadae6a5ac-0', usage_metadata={'input_tokens': 45, 'output_tokens': 55, 'total_tokens': 100})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_obj.invoke([HumanMessage(content=\"Hello my name is Karthik I am instructor\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "eacb628e-f43e-4777-b886-16bdd9dc15a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have the ability to know your name as our conversation has just begun. I'm a text-based AI assistant and we don't have a prior knowledge base or personal interactions. Would you like to share your name with me?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 40, 'total_tokens': 88, 'completion_time': 0.077054965, 'prompt_time': 0.001862318, 'queue_time': 0.050377492, 'total_time': 0.078917283}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--72d7ac28-dd0b-4018-b804-6bf8032764a2-0', usage_metadata={'input_tokens': 40, 'output_tokens': 48, 'total_tokens': 88})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_obj.invoke([HumanMessage(content=\"Hi Tell me my name\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f7017-e48b-4c51-bd5a-47eba01bb85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "User_Q: What is python\n",
    "AI_R: .....\n",
    "User_Q: give me any 5 features\n",
    "AI_R: there are 5 import python features\n",
    "----------------\n",
    "User_Q1: Hello my name is Tom\n",
    "AI_ Hello Tom ..my name is userA......\n",
    "User_Q2: renewal my website domain\n",
    "AI  Tom ...type your website domain\n",
    "User_Q2:\n",
    "-----------------------------------//session expired \n",
    "User_Q1: I will type website name?\n",
    "AI_ may i know ...\n",
    "...\n",
    "--------------------------------------//session expired\n",
    "\n",
    " +------------------------------------------+\n",
    " | sessionID        |    chat(Human+AI)     |\n",
    " +------------------+-----------------------+\n",
    " |   Session1       | User_Q: What is python \n",
    "                      AI_R: .....\n",
    "                      User_Q: give me any 5 features\n",
    "                      AI_R: there are 5 import python features\n",
    " |------------------------------------------+\n",
    " |   Session2:      |  User_Q1: Hello my name is Tom\n",
    "                    |  AI_ Hello Tom ..my name is userA......\n",
    "                    |  User_Q2: renewal my website domain\n",
    "                    |  AI  Tom ...type your website domain\n",
    " |\n",
    " |------------------|-----------------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ac3d9-8165-4412-ac26-9842124a1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################  End of Day3 ############################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
