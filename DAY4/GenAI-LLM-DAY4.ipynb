{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17eaf8a-f043-4375-83b0-631698db49c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p101': 'pA', 'p102': 'pB', 'p103': 'pC'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores = {}\n",
    "stores['p101'] = 'pA'\n",
    "stores['p102'] = 'pB'\n",
    "stores['p103'] = 'pC'\n",
    "\n",
    "stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d348e38f-c40f-4a7f-8f5d-fa80d76e46d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p101': ['pA', 1000, 'pVendor1'], 'p102': ['pB', 2000, 'pVendor2']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores = {}\n",
    "stores ['p101'] = ['pA',1000,'pVendor1']\n",
    "stores ['p102'] = ['pB',2000,'pVendor2']\n",
    "\n",
    "stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10cf0b96-bd23-4d22-bf39-d4d1fda6bc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pB', 2000, 'pVendor2']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores['p102']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b91b93-0736-49cf-88a1-8a8e8cdb7f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatMessageHistory - stores messages (Human + AI )\n",
    "|\n",
    "BaseChatMessageHistory - Abstract base class(interface) \n",
    "|\n",
    "RunnableWithMessageHistory - Langchain wrapper - combines an LLM with messagehistory \n",
    "-----------------------------//\n",
    "\n",
    "stores ={\n",
    "    \"session_1\" : ChatMessageHistory(),\n",
    "    \"session_2\" : ChatMessageHistory(),\n",
    "     ...\n",
    "}\n",
    "def user_defined_function(session_id:str) -> BaseChatMessageHistory:\n",
    "      if session_id not in stores:\n",
    "          stores[session_id] = ChatMessageHistory()\n",
    "      return stores[session_id]\n",
    "\n",
    "obj = RunnableWithMessageHistory(llm_obj,user_defined_function)\n",
    "\n",
    "obj.invoke({'message':[role],config={\"configurable\":{\"session_id\":<sessionID>}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13bd1ab-fc89-4ad7-8002-0ab3a91e6e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "User_Q: What is python\n",
    "AI_R: .....\n",
    "User_Q: give me any 5 features\n",
    "AI_R: there are 5 import python features\n",
    "----------------\n",
    "User_Q1: Hello my name is Tom\n",
    "AI_ Hello Tom ..my name is userA......\n",
    "User_Q2: renewal my website domain\n",
    "AI  Tom ...type your website domain\n",
    "User_Q2:\n",
    "-----------------------------------//session expired \n",
    "User_Q1: I will type website name?\n",
    "AI_ may i know ...\n",
    "...\n",
    "--------------------------------------//session expired\n",
    "\n",
    " +------------------------------------------+\n",
    " | sessionID        |    chat(Human+AI)     |\n",
    " +------------------+-----------------------+\n",
    " |   Session1       | User_Q: What is python \n",
    "                      AI_R: .....\n",
    "                      User_Q: give me any 5 features\n",
    "                      AI_R: there are 5 import python features\n",
    " |------------------------------------------+\n",
    " |   Session2:      |  User_Q1: Hello my name is Tom\n",
    "                    |  AI_ Hello Tom ..my name is userA......\n",
    "                    |  User_Q2: renewal my website domain\n",
    "                    |  AI  Tom ...type your website domain\n",
    " |\n",
    " |------------------|-----------------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9759f7ce-b30a-40b4-9c0a-7846dc755a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a943cc-7878-4f15-8abe-20b2c6bfd350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da51d81c-4fa0-4297-a25b-c433a6be9ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccb3370e-74f3-4997-b57e-c394fff523d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_obj = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "809853bf-4387-4c50-9b91-abc673b1b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = {}\n",
    "def get_session_history(session_id: str) ->BaseChatMessageHistory:\n",
    "    if session_id not in stores:\n",
    "        stores[session_id] = ChatMessageHistory()\n",
    "    return stores[session_id]\n",
    "\n",
    "msg_history = RunnableWithMessageHistory(llm_obj,get_session_history)\n",
    "my_config = {\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "285c07eb-2a6f-4073-8f40-b74620fd7d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello Tom, it's nice to meet you. Is there anything I can help you with or would you like to chat?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 41, 'total_tokens': 67, 'completion_time': 0.029467624, 'prompt_time': 0.001960013, 'queue_time': 0.054479717, 'total_time': 0.031427637}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--a6eb2840-6c7e-4ebb-a3e1-4e7009decdc0-0' usage_metadata={'input_tokens': 41, 'output_tokens': 26, 'total_tokens': 67}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "response = msg_history.invoke([HumanMessage(content=\"Hello, My name is Tom\")],config=my_config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71a346f4-aebe-45c4-87bd-4b5d3dd4ddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Tom, it's nice to meet you. Is there anything I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3b027da-c3bb-4f3c-9e3b-b45bd12f99ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As I mentioned earlier, I don't have any information about your personal details. You haven't shared your name with me, and I don't retain any information from previous conversations.\n",
      "\n",
      "If you'd like to introduce yourself, I'd be happy to chat with you and address you by your chosen name.\n"
     ]
    }
   ],
   "source": [
    "my_config = {\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response = msg_history.invoke([HumanMessage(content=\"What is my name\")],config=my_config)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80cf03d3-e457-4ba7-85d3-0c0e3c516c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Tom.\n"
     ]
    }
   ],
   "source": [
    "my_config = {\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "response = msg_history.invoke([HumanMessage(content=\"What is my name\")],config=my_config)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "401328d6-2882-4bc0-8949-eade902ccd45",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sample Python Program: A Simple Calculator**\n",
      "=====================================================\n",
      "\n",
      "Here's a simple example of a Python program that acts as a basic calculator. This program will take an operation and two numbers as input from the user and then perform the operation.\n",
      "\n",
      "### Code\n",
      "```python\n",
      "# Import the necessary module for getting user input\n",
      "import operator\n",
      "\n",
      "# Define a dictionary that maps operations to their corresponding functions\n",
      "operations = {\n",
      "    '+': operator.add,\n",
      "    '-': operator.sub,\n",
      "    '*': operator.mul,\n",
      "    '/': operator.truediv\n",
      "}\n",
      "\n",
      "def calculator():\n",
      "    # Get the operation from the user\n",
      "    print(\"Available operations:\")\n",
      "    print(\"+ for addition\")\n",
      "    print(\"- for subtraction\")\n",
      "    print(\"* for multiplication\")\n",
      "    print(\"/ for division\")\n",
      "    op = input(\"Enter the operation: \")\n",
      "\n",
      "    # Check if the operation is valid\n",
      "    if op not in operations:\n",
      "        print(\"Invalid operation. Please try again.\")\n",
      "        return\n",
      "\n",
      "    # Get the two numbers from the user\n",
      "    num1 = float(input(\"Enter the first number: \"))\n",
      "    num2 = float(input(\"Enter the second number: \"))\n",
      "\n",
      "    # Perform the operation\n",
      "    try:\n",
      "        result = operations[op](num1, num2)\n",
      "        print(f\"{num1} {op} {num2} = {result}\")\n",
      "    except ZeroDivisionError:\n",
      "        print(\"Error: Division by zero is not allowed.\")\n",
      "\n",
      "# Run the calculator\n",
      "calculator()\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "\n",
      "1. We import the `operator` module, which provides functions for basic mathematical operations.\n",
      "2. We define a dictionary `operations` that maps operation symbols to their corresponding functions.\n",
      "3. We define a function `calculator()` that will take care of getting user input, performing the operation, and displaying the result.\n",
      "4. Inside the `calculator()` function, we get the operation from the user and check if it's valid.\n",
      "5. We then get the two numbers from the user and perform the operation using the corresponding function from the `operations` dictionary.\n",
      "6. If the operation is division and the second number is zero, we catch the `ZeroDivisionError` and display an error message.\n",
      "7. Finally, we call the `calculator()` function to start the program.\n",
      "\n",
      "### Example Use Cases\n",
      "\n",
      "* Run the program and enter `+` as the operation and `10` and `5` as the numbers. The program will display `10 + 5 = 15`.\n",
      "* Run the program and enter `*` as the operation and `10` and `5` as the numbers. The program will display `10 * 5 = 50`.\n",
      "* Run the program and enter `/` as the operation and `10` and `0` as the numbers. The program will display `Error: Division by zero is not allowed.`.\n"
     ]
    }
   ],
   "source": [
    "my_config = {\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response = msg_history.invoke([HumanMessage(content=\"How to write sample python program?\")],config=my_config)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9460173e-223d-40af-9ac9-baedc48af1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['chat1', 'chat2', 'chat3'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "stores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d9e3d7d-d0b7-4d61-bc7c-1dde79d42e18",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InMemoryChatMessageHistory(messages=[HumanMessage(content='Hello, My name is Tom', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Tom, it's nice to meet you. Is there anything I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 41, 'total_tokens': 67, 'completion_time': 0.029467624, 'prompt_time': 0.001960013, 'queue_time': 0.054479717, 'total_time': 0.031427637}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--a6eb2840-6c7e-4ebb-a3e1-4e7009decdc0-0', usage_metadata={'input_tokens': 41, 'output_tokens': 26, 'total_tokens': 67}), HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Tom.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 80, 'total_tokens': 86, 'completion_time': 0.005235767, 'prompt_time': 0.00468274, 'queue_time': 0.053428459, 'total_time': 0.009918507}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--87289ea0-90fc-4c9f-9fd5-57727cd11fe1-0', usage_metadata={'input_tokens': 80, 'output_tokens': 6, 'total_tokens': 86})])\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(stores['chat1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02a2146d-b174-4dfd-b7f3-21cb898e1826",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq is a company that provides high-performance AI and machine learning (ML) solutions, but I'm assuming you're looking for free LLM (Large Language Model) models, not directly from Groq. Groq doesn't provide free LLM models.\n",
      "\n",
      "However, there are a few alternatives where you can find or train your own free LLM models. Here are a few options:\n",
      "\n",
      "1. **Hugging Face Transformers**: You can download pre-trained models, including LLMs, from the Hugging Face model hub. They also provide a library for fine-tuning and training your own models.\n",
      "2. **Transformers by Facebook AI**: This is another popular library for natural language processing, including LLMs. You can download pre-trained models or train your own models.\n",
      "3. **Google's BERT and ALBERT models**: You can download pre-trained LLM models like BERT and ALBERT from Google's research repository.\n",
      "4. **Microsoft's Turing-NLG model**: This is a large language model developed by Microsoft Research. You can download and use the pre-trained model.\n",
      "5. **Open-source LLM models**: There are several open-source LLM models available on GitHub, such as the \"Big Bird\" model.\n",
      "\n",
      "Keep in mind that these models may not be as powerful as commercial LLMs, like those used by Groq, but they can still be very useful for many NLP tasks.\n",
      "\n",
      "If you're interested in fine-tuning or training your own LLM models, you'll need to have a good understanding of ML concepts, computational resources (e.g., GPU), and software tools like TensorFlow, PyTorch, or Hugging Face Transformers.\n",
      "\n",
      "Let me know if you need more information or help with getting started.\n"
     ]
    }
   ],
   "source": [
    "my_config = {\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "response = msg_history.invoke([HumanMessage(content=\"help me to get free llm models from groq\")],config=my_config)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "522e16ab-09a4-4736-ab3c-4a420fe96cf6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InMemoryChatMessageHistory(messages=[HumanMessage(content='Hello, My name is Tom', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Tom, it's nice to meet you. Is there anything I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 41, 'total_tokens': 67, 'completion_time': 0.029467624, 'prompt_time': 0.001960013, 'queue_time': 0.054479717, 'total_time': 0.031427637}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--a6eb2840-6c7e-4ebb-a3e1-4e7009decdc0-0', usage_metadata={'input_tokens': 41, 'output_tokens': 26, 'total_tokens': 67}), HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Tom.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 80, 'total_tokens': 86, 'completion_time': 0.005235767, 'prompt_time': 0.00468274, 'queue_time': 0.053428459, 'total_time': 0.009918507}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--87289ea0-90fc-4c9f-9fd5-57727cd11fe1-0', usage_metadata={'input_tokens': 80, 'output_tokens': 6, 'total_tokens': 86}), HumanMessage(content='help me to get free llm models from groq', additional_kwargs={}, response_metadata={}), AIMessage(content='Groq is a company that provides high-performance AI and machine learning (ML) solutions, but I\\'m assuming you\\'re looking for free LLM (Large Language Model) models, not directly from Groq. Groq doesn\\'t provide free LLM models.\\n\\nHowever, there are a few alternatives where you can find or train your own free LLM models. Here are a few options:\\n\\n1. **Hugging Face Transformers**: You can download pre-trained models, including LLMs, from the Hugging Face model hub. They also provide a library for fine-tuning and training your own models.\\n2. **Transformers by Facebook AI**: This is another popular library for natural language processing, including LLMs. You can download pre-trained models or train your own models.\\n3. **Google\\'s BERT and ALBERT models**: You can download pre-trained LLM models like BERT and ALBERT from Google\\'s research repository.\\n4. **Microsoft\\'s Turing-NLG model**: This is a large language model developed by Microsoft Research. You can download and use the pre-trained model.\\n5. **Open-source LLM models**: There are several open-source LLM models available on GitHub, such as the \"Big Bird\" model.\\n\\nKeep in mind that these models may not be as powerful as commercial LLMs, like those used by Groq, but they can still be very useful for many NLP tasks.\\n\\nIf you\\'re interested in fine-tuning or training your own LLM models, you\\'ll need to have a good understanding of ML concepts, computational resources (e.g., GPU), and software tools like TensorFlow, PyTorch, or Hugging Face Transformers.\\n\\nLet me know if you need more information or help with getting started.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 354, 'prompt_tokens': 106, 'total_tokens': 460, 'completion_time': 0.610434781, 'prompt_time': 0.006019447, 'queue_time': 0.050785583, 'total_time': 0.616454228}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--20ffa832-776d-4df9-a96e-72190e908e8e-0', usage_metadata={'input_tokens': 106, 'output_tokens': 354, 'total_tokens': 460})])\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(stores['chat1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c7b2f6d-303c-4e34-b04e-c09bbb4f4382",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chat_history.log','a') as wobj:\n",
    "    for var in stores:\n",
    "        wobj.write(var+str(stores[var])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b9f90bb-b89a-4ae4-a56b-4e716205689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1aaf2ae7-f473-4d8b-8248-85a280d71047",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your are helpful AI assistant,Answer all the question in {language}.\"),\n",
    "    MessagesPlaceholder(variable_name=\"question\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa575019-69a7-40f1-a9c8-a58a9a39e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt|llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "968d4327-c64b-4cd5-b54a-ae8607b7a19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bonjour Leo ! Enchanté de faire votre connaissance ! Comment puis-je vous aider aujourd'hui ?\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({'question':[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"french\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35121a2f-6589-4ddd-8b2f-3bf0d50dbf83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते लियो! मैं आपकी मदद करने के लिए यहां हूं। क्या मैं आपकी कोई समस्या का समाधान कर सकता हूं या आपके लिए कोई जानकारी प्रदान कर सकता हूं?'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({'question':[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77ca9ef7-326a-420a-a7a5-df8622f190d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'హలో లెయో! నేను సహాయక కంప్యూటర్ అసిస్టెంట్. నీవు ఏ సమస్యను లేదా అంశాన్ని పరిష్కరించాలనుకుంటున్నావా?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({'question':[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"telugu\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "012b8606-a3c8-4dfc-b944-26b2b43adb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'வணக்கம் லியோ! நான் உங்களுக்கு உதவ இங்கு இருக்கிறேன். உங்களுக்கு என்ன தேவைப்படுகிறது?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({'question':[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"tamil\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dab26fd-b755-4124-8a6d-9cebd569c0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ನಿಮ್ಮ ಹೆಸರು ಲಿಯೊ. ನಾನು ನಿಮಗಿಡಲು ಸಹಾಯ ಮಾಡಬೇಕಾಗುತ್ತದೆ, ನೀವು ಯಾವ ಸಮಸ್ಯೆಯ ಬಗ್ಗೆ ಪ್ರಶ್ನಿಸುತ್ತೀರೆ?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({'question':[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"kannada\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39215512-5e47-462b-aea8-3a0a51f383d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ನಿಮ್ಮ ಹೆಸರು ಲಿಯೊ. ನಾನು ನಿಮಗಿಡಲು ಸಹಾಯ ಮಾಡಬೇಕಾಗುತ್ತದೆ, ನೀವು ಯಾವ ಸಮಸ್ಯೆಯ ಬಗ್ಗೆ ಪ್ರಶ್ನಿಸುತ್ತೀರೆ?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 160, 'prompt_tokens': 54, 'total_tokens': 214, 'completion_time': 0.21999512, 'prompt_time': 0.002638934, 'queue_time': 0.055482936, 'total_time': 0.222634054}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--133a6fa7-6f1a-4621-9599-f11ccff732a3-0', usage_metadata={'input_tokens': 54, 'output_tokens': 160, 'total_tokens': 214})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1299df-cd8d-448f-98c7-d58564b8c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "Core ideas of prompt engineering\n",
    "|-> llm  - follow the prompt rules and role\n",
    "\n",
    "shots - an example \n",
    "\n",
    "1. zero-shot prompt ==> No examples given\n",
    "Example\n",
    "--------\n",
    "prompt:  \"Translate the following english sentence to french\"\n",
    "         \"I am learning artifical intelligence\"\n",
    "\n",
    "Model output: \".\"Enchanté de faire votre connaissance\"\n",
    "\n",
    "2. one-shot prompt --> One example given\n",
    "prompt:  \"Translate the following english sentence to french\"\n",
    "         French: \"nchanté de faire votre connaissan\"\n",
    "         Now translate:\n",
    "         English: \"I am learning artifical intelligence\"\n",
    "Model output: \".\"Enchanté de faire votre connaissance\"\n",
    "\n",
    "3. few-shot prompt - 2 or 5 exampels (or) more that\n",
    "Prompt:\n",
    "Classify the sentiment of each sentence as Positive,Negative,Neutal\n",
    "\n",
    "Example 1:\n",
    "sentence: The movie was fantastic \n",
    "sentiment: positive\n",
    "Example 2:\n",
    "sentence: The food was not good\n",
    "sentiment: negative\n",
    "Example 3:\n",
    "sentence: It was an average day\n",
    "sentiment: Neutral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f01a4a75-06e8-4cdd-a51e-52f5c6ce4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = ChatPromptTemplate.from_template('''\n",
    "classify the sentiment of the given text as positive,negative or neutral\n",
    "text:{input}\n",
    "Sentiment:''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e02b8270-a5fc-411c-9dea-ed4c35233917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sentiment: Positive\\n\\nThe text contains words and phrases with extremely positive connotations, such as \"I absolutely loved\" and \"fantastic\", indicating a strong positive sentiment towards the movie.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 63, 'total_tokens': 102, 'completion_time': 0.066417411, 'prompt_time': 0.00354582, 'queue_time': 0.05402879, 'total_time': 0.069963231}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--01139dbc-ab6e-4d48-b4bb-f9c35bfe2f14-0', usage_metadata={'input_tokens': 63, 'output_tokens': 39, 'total_tokens': 102})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot = zero_shot_prompt.format_messages(input=\"I absolutely loved the movie, it was fantastic!\")\n",
    "llm_obj.invoke(zero_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f25bfa6-f78d-434f-aaad-0bdf84cba80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The sentiment of the text is Negative.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 61, 'total_tokens': 70, 'completion_time': 0.014569001, 'prompt_time': 0.004444951, 'queue_time': 0.055270859, 'total_time': 0.019013952}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2a70fc44-09ba-433e-8594-b4e124ae6258-0', usage_metadata={'input_tokens': 61, 'output_tokens': 9, 'total_tokens': 70})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot = zero_shot_prompt.format_messages(input=\"I not loved the movie\")\n",
    "llm_obj.invoke(zero_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89732de0-d226-413a-a0dc-ec40e4a8de06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sentiment: Neutral\\n\\nThe word \"ok\" is a neutral expression that doesn\\'t convey strong emotions, indicating a lukewarm or average opinion about the movie.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 59, 'total_tokens': 93, 'completion_time': 0.056174522, 'prompt_time': 0.00294793, 'queue_time': 0.05543042, 'total_time': 0.059122452}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--981f5844-18ce-4c1f-846a-9a8e2e920880-0', usage_metadata={'input_tokens': 59, 'output_tokens': 34, 'total_tokens': 93})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot = zero_shot_prompt.format_messages(input=\"the movie was ok\")\n",
    "llm_obj.invoke(zero_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e42655-3761-4970-8f4a-6525dae2df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task - Summarization from loaded pdf file\n",
    "1. Loads a pdf file\n",
    "2. Splits into chunks\n",
    "3. Create vector embedding\n",
    "4. Stores to vector\n",
    "5. use RAG to summarize the pdf\n",
    "     retriever_obj = vectorstore.as_retriever()\n",
    "\n",
    "   5.1 ->create llm model\n",
    "   5.2 -> Build RAG chain (retriever + llm)\n",
    "          qa_chain = RetriverQA.from_chain_type(llm=<llm_obj>,retriever=<retriver_obj>)\n",
    "   5.3 -> Ask summarization\n",
    "            |\n",
    "          user_query='Summarize the results from this pdf\"\n",
    "          qa_chain.invoke('query':user_query) ->response \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ed70aa9-ce08-4ecd-8a7c-e042eca43ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "076d1c19-36d4-41b2-af4a-06c270dabfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step-1 Load pdf file\n",
    "loader = PyPDFLoader(\"attention.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee3d833e-5a49-4cca-9cb3-9332bbcf68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step-2 split text into chunks \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e0577c6-2350-4eac-958d-05dff0a82238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step-3 create embedding\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1058e1bc-9863-4446-97ae-d125fbde2d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# step-4 Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "retriever_obj = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa59fd5a-10f2-45da-ab85-04cd3928dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step-5 create a llm\n",
    "llm_obj = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f70aca6c-fe4b-403f-8682-c7f3b865b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step-6 Build RAG chain (retirver + llm)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ece66d7-61ec-4357-909c-81abe877ab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# step 7 ask summarization\n",
    "query_input = \"Summarize the results from this pdf content\"\n",
    "response = qa_chain.invoke({\"query\":query_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10eea87f-765c-4bfc-8e09-4136b7395f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't see a pdf content provided. However, I can summarize the results based on the text you provided.\n",
      "\n",
      "The text appears to be a research paper on deep learning, specifically on a model called \"Attention Is All You Need\". The paper presents results from experiments on a machine translation task.\n",
      "\n",
      "The main results are:\n",
      "\n",
      "* The best performing model uses 8 attention heads with key and value dimensions of 1024.\n",
      "* Varying the number of attention heads affects the model's performance, with single-head attention being 0.9 BLEU worse than the best setting.\n",
      "* Increasing the number of attention heads beyond 8 does not improve performance.\n",
      "* The development set results are presented in Table 3, but the actual results are not shown in the text.\n",
      "\n",
      "Please provide the actual pdf content if you would like a more detailed summary.\n"
     ]
    }
   ],
   "source": [
    "# step 8: display results\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6f3432e6-6048-40c7-8402-3addf433ee21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "## step 7 - grammar,spelling check \n",
    "query_input = '''\n",
    "You are a professional english editor\n",
    "Correct grammar,punctuation and sentence flow in the following text from the pdf and improve readability.'''\n",
    "response = qa_chain.invoke({\"query\":query_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fff52c63-d4e9-4e8c-adbd-c3e39e234d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, I don't see the text from the pdf. Please paste the text you'd like me to correct and improve, and I'll get started.\n",
      "\n",
      "If you'd like to provide the context from the references you provided earlier, I can also use those as a reference to ensure consistent formatting and style.\n"
     ]
    }
   ],
   "source": [
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b172f72e-b96b-483f-bec5-bd8ea8f19303",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': None,\n",
       " 'memory': None,\n",
       " 'callbacks': None,\n",
       " 'verbose': False,\n",
       " 'tags': None,\n",
       " 'metadata': None,\n",
       " 'callback_manager': None,\n",
       " 'combine_documents_chain': StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001E298D90E10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001E298D920D0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context'),\n",
       " 'input_key': 'query',\n",
       " 'output_key': 'result',\n",
       " 'return_source_documents': False,\n",
       " 'retriever': VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001E2940EDA90>, search_kwargs={})}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7a42358d-da23-41ba-b643-e418be1b570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "class product:\n",
    "    '''this is sample product inventory'''\n",
    "    prod_id = 0\n",
    "    prod_name = 'sample'\n",
    "    prod_cost = 0.0\n",
    "    def method1(self,pid,pn,pcost):\n",
    "        '''sample method returns product details'''\n",
    "        self.prod_id = pid\n",
    "        self.prod_name = pn\n",
    "        self.prod_cost = pcost\n",
    "        return self.prod_id,self.prod_name,self.prod_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ff1d24c-cfd7-4e81-ad35-bf76d32bc539",
   "metadata": {},
   "outputs": [],
   "source": [
    "myobj = product()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2e97ed4c-b815-4020-b056-e32f12a331f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.product at 0x1e2940ee120>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "76b2f798-a2fd-4b5c-8157-aa0d00c10c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dir(myobj)\n",
    "myobj.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9ce2a320-13e8-495f-86a9-ac2aca2ac786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 'pA', 1000)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myobj.method1(101,'pA',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b2011e0a-afaf-413a-bcd0-01d4629726fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prod_id': 101, 'prod_name': 'pA', 'prod_cost': 1000}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myobj.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c2706ffc-2ed0-4e44-a7d9-2f4996e4100a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 'pB', 2000)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myobj.method1(102,'pB',2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0c17f692-9399-4ddd-a19c-579a38677547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prod_id': 102, 'prod_name': 'pB', 'prod_cost': 2000}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myobj.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "55b79a19-268f-4346-a3ec-626bba34ffea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': '__main__',\n",
       "              '__firstlineno__': 2,\n",
       "              '__doc__': 'this is sample product inventory',\n",
       "              'prod_id': 0,\n",
       "              'prod_name': 'sample',\n",
       "              'prod_cost': 0.0,\n",
       "              'method1': <function __main__.product.method1(self, pid, pn, pcost)>,\n",
       "              '__static_attributes__': ('prod_cost', 'prod_id', 'prod_name'),\n",
       "              '__dict__': <attribute '__dict__' of 'product' objects>,\n",
       "              '__weakref__': <attribute '__weakref__' of 'product' objects>})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "65253237-9670-4b96-a3c7-ba477ffe2414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is sample product inventory'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fc4a01d6-e194-48ed-978b-a2000a945aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and parse a PDF file using 'pypdf' library.\n",
      "\n",
      "    This class provides methods to load and parse PDF documents, supporting various\n",
      "    configurations such as handling password-protected files, extracting images, and\n",
      "    defining extraction mode. It integrates the `pypdf` library for PDF processing and\n",
      "    offers both synchronous and asynchronous document loading.\n",
      "\n",
      "    Examples:\n",
      "        Setup:\n",
      "\n",
      "        .. code-block:: bash\n",
      "\n",
      "            pip install -U langchain-community pypdf\n",
      "\n",
      "        Instantiate the loader:\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain_community.document_loaders import PyPDFLoader\n",
      "\n",
      "            loader = PyPDFLoader(\n",
      "                file_path = \"./example_data/layout-parser-paper.pdf\",\n",
      "                # headers = None\n",
      "                # password = None,\n",
      "                mode = \"single\",\n",
      "                pages_delimiter = \"\n",
      "\f",
      "\",\n",
      "                # extract_images = True,\n",
      "                # images_parser = RapidOCRBlobParser(),\n",
      "            )\n",
      "\n",
      "        Lazy load documents:\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            docs = []\n",
      "            docs_lazy = loader.lazy_load()\n",
      "\n",
      "            for doc in docs_lazy:\n",
      "                docs.append(doc)\n",
      "            print(docs[0].page_content[:100])\n",
      "            print(docs[0].metadata)\n",
      "\n",
      "        Load documents asynchronously:\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            docs = await loader.aload()\n",
      "            print(docs[0].page_content[:100])\n",
      "            print(docs[0].metadata)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(PyPDFLoader.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa231d7b-0a88-4a0d-8db0-29aefff578aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5793f6-864a-45f4-9c73-83e217ec57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt|llmobject + chain.invoke({'question':....})\n",
    "=====   ===================\n",
    "           |-->generate results based on retriver results\n",
    "\n",
    "create_stuff_documents_chain (llm,prompt)->qa_chain\n",
    "response = chain.invoke({'input_documents':docs,'question':'what is langchain'})\n",
    "Vs\n",
    "create_retrival_chain(retriver_object,qa_chain)\n",
    "query ->retriver -->llm \n",
    "\n",
    "\n",
    "vectorstore.as_retriver() -> retriver_obj  -->returns documents that match the user question/query\n",
    "\n",
    "qachain \n",
    "-------\n",
    "create_stuff_documents_chain(llm_object,prompt) ->qa_chain \n",
    "create_retrieval_chain(retriever,qa_chain) -->rag_chain  ==> rag_chain.invoke({'input':'what is langchain?'})\n",
    "\n",
    "user_question\n",
    " |\n",
    "Retriver(fetches relevant docs from db)\n",
    "|\n",
    "QA Chain\n",
    "|\n",
    "FinalAnswer \n",
    "\n",
    "context - predefined key attribute\n",
    "|\n",
    "|->response content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed15bff5-99d8-49da-b978-47f080af501e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4a1ce247-e4c2-4d1c-ad4e-03aa48739032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "08a2a377-1738-4aa7-b434-e8b06d159292",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'what is langchain?', 'context': [Document(id='8644d6fe-5eb8-4b65-ba7b-c540baf74369', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.'), Document(id='054b810c-e1d3-4ac6-8ea1-08b95c98874d', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='language modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture'), Document(id='2802cc57-c105-457f-9761-4382e8b2e20f', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts'), Document(id='d698104e-5bc3-4375-a558-576e2d5a5210', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000')], 'answer': 'The provided text does not mention \"LangChain\". However, based on the context and available information, it appears to be a discussion about the Transformer model, a type of neural network architecture used for natural language processing tasks.\\n\\nLangChain, on the other hand, is a different entity. LangChain is an open-source framework for building large language models and multimodal AI applications. It is designed to be a modular and flexible platform for developing and integrating various components, such as language models, text generation, and dialogue management.\\n\\nIn more detail, LangChain was created by Joshua Browning and is now maintained by the LangChain team. It is a Python-based framework that allows developers to build and train their own language models, as well as integrate them with other tools and services. LangChain supports various language models, including the popular LLaMA and BLOOM models.\\n\\nTo use LangChain, developers typically follow these steps:\\n\\n1. Install the framework using pip or a package manager.\\n2. Choose a language model or build their own from scratch.\\n3. Use the LangChain API to integrate the model with other components, such as a dialogue manager or a text generation module.\\n4. Train and fine-tune the model using a large dataset or a specific task.\\n\\nOverall, LangChain is a powerful tool for building and deploying large language models and multimodal AI applications.'}\n"
     ]
    }
   ],
   "source": [
    "# step-1 Load pdf file\n",
    "loader = PyPDFLoader(\"attention.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# step-2 split text into chunks \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# step-3 create embedding\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# step-4 Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "retriever_obj = vectorstore.as_retriever()\n",
    "\n",
    "# step-5 create a llm\n",
    "llm_obj = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "## prompt + chain\n",
    "prompt = ChatPromptTemplate.from_template(\"Use the context to answer:{context}\\n:Question:{input}\")\n",
    "qa_chain = create_stuff_documents_chain(llm_obj,prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever_obj,qa_chain)\n",
    "\n",
    "## User query\n",
    "response = rag_chain.invoke({'input':'what is langchain?'})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dfd648-0b71-41c5-8f2a-9db4e157e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "User Question -->[Retriver_object] -->[fetch the top matchedchunks] ->document_chain -->{context} and {input} in prompt ->LLM ->finalAnswer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a44e1-2396-4b8f-8407-6af11c8f43d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2496bb05-e08c-4b8e-a56d-c40fcaec91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------\n",
    "# strict RAG ==> Avoid hallucination only answer from docs \n",
    "#\n",
    "# Hybrid RAG ==> answer from docs + fallback to llm knowledge \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b43dc-72eb-48f6-a7f5-7bf4cd8c56b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strict RAG\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\",\"question\"],\n",
    "    template=\"\"\"\n",
    "    You are a strict retrieval QA assistant.\n",
    "    Answer only using the information given in the context below.\n",
    "    If the answer is not present in the context, reply exactly with:\n",
    "    \" I don't know, the document does not contain this information.\"\n",
    "     Context:\n",
    "     {context}\n",
    "     Question:\n",
    "     {question}\n",
    "     Answer:\n",
    "     \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae59bc-5e38-457d-a964-f9b91a90bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid RAG\n",
    "prompt = PromptTemplate(\n",
    "    Use the follwoing context to answer the question.\n",
    "    If the answer is not present in the context, answer from your general knowledge.\n",
    "     Context:\n",
    "     {context}\n",
    "     Question:\n",
    "     {question}\n",
    "     Answer:\n",
    "     \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5497bb73-0285-4d9b-9d38-83848c0de7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(RetrievalQA.from_chain_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e6d4bf3a-a160-4991-b9cd-c258ca94b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader \n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e06d804a-ad05-4c40-9860-0a9533e327b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Step-1 load the data\n",
    "loader = TextLoader('my_docs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "#Step-2 split - chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "#Step-3 embeddingobject\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "#Step-4 stores to vectorDB\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "#Step-5 similarity_search ==> retriver_object\n",
    "#vectorstore.similarity_search(\"what is langchain?\")\n",
    "retriever_obj = vectorstore.as_retriever() # Convert vector stores into a retriver object\n",
    "\n",
    "#Step-6: create llm objct\n",
    "llm_obj = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "# strict RAG\n",
    "my_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\",\"question\"],\n",
    "    template=\"\"\"\n",
    "    You are a strict retrieval QA assistant.\n",
    "    Answer only using the information given in the context below.\n",
    "    If the answer is not present in the context, reply exactly with:\n",
    "    \" I don't know, the document does not contain this information.\"\n",
    "     Context:\n",
    "     {context}\n",
    "     Question:\n",
    "     {question}\n",
    "     Answer:\n",
    "     \"\"\"\n",
    ")\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm_obj,\n",
    "    retriever = retriever_obj,\n",
    "    chain_type= \"stuff\",\n",
    "    chain_type_kwargs={\"prompt\":my_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a36c081a-0323-4d2e-b691-31d477167e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_6896\\4218938368.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(rag_chain(\"How to write factorial program in C language?\"))\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How to write factorial program in C language?', 'result': \"I don't know, the document does not contain this information.\"}\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain(\"How to write factorial program in C language?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2a6c86d2-451d-40ef-af22-fa3b76942d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies every stage of the LLM application lifecycle, including development, productionization, and deployment.\\n\\nHowever, the document does not contain information on how to write a factorial program. It only provides information about LangChain and its features.\\n\\nTo write a factorial program, you would typically need to use a programming language such as Python, JavaScript, or C++. The document does not provide any specific instructions for writing a factorial program, but a simple example in Python could be:\\n\\n```python\\ndef factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n```\\n\\nThis code defines a recursive function `factorial` that calculates the factorial of a given number `n`. However, I don't know if this code is part of the LangChain framework, as the document does not contain any information on programming or specific code examples. \\n\\nTo write a factorial program within the context of LangChain, you would need to use LangChain's open-source components and third-party integrations, and this would likely involve using a programming language.\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain(\"what is langchain and how to write factorial program?\")['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931c00f-727e-48b6-bd82-92d6d251c8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9c47ec-357e-4b00-99ef-bf315ce0b36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ca7a44de-2d0f-475a-92a0-5a1ae0aee7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what is langchain?', 'result': 'LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies every stage of the LLM application lifecycle, including development, productionization, and deployment.'}\n"
     ]
    }
   ],
   "source": [
    "#Step-1 load the data\n",
    "loader = TextLoader('my_docs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "#Step-2 split - chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "#Step-3 embeddingobject\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "#Step-4 stores to vectorDB\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "#Step-5 similarity_search ==> retriver_object\n",
    "#vectorstore.similarity_search(\"what is langchain?\")\n",
    "retriever_obj = vectorstore.as_retriever() # Convert vector stores into a retriver object\n",
    "\n",
    "#Step-6: create llm objct\n",
    "llm_obj = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "# Hybrid RAG\n",
    "my_template=\"\"\"\n",
    "    Use the follwoing context to answer the question.\n",
    "    If the answer is not present in the context, answer from your general knowledge.\n",
    "     Context:\n",
    "     {context}\n",
    "     Question:\n",
    "     {question}\n",
    "     Answer:\"\"\"\n",
    "my_prompt = PromptTemplate(input_variables=[\"context\",\"question\"],template=my_template)\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm_obj,\n",
    "    retriever = retriever_obj,\n",
    "    chain_type= \"stuff\",\n",
    "    chain_type_kwargs={\"prompt\":my_prompt})\n",
    "\n",
    "## Question\n",
    "print(rag_chain(\"what is langchain?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2de4ed8f-e8dd-433c-b39e-45868b690aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How to write factorial program in C language?', 'result': 'The provided context is related to LangChain, a framework for developing applications powered by large language models (LLMs), and does not contain any information about writing a factorial program in C language.\\n\\nHowever, I can provide the answer from my general knowledge.\\n\\nHere\\'s a simple factorial program in C language:\\n\\n```c\\n#include <stdio.h>\\n\\nint factorial(int n) {\\n    if (n == 0) {\\n        return 1;\\n    } else {\\n        return n * factorial(n-1);\\n    }\\n}\\n\\nint main() {\\n    int num;\\n    printf(\"Enter a number: \");\\n    scanf(\"%d\", &num);\\n    printf(\"Factorial of %d is: %d\\\\n\", num, factorial(num));\\n    return 0;\\n}\\n```\\n\\nThis program calculates the factorial of a given number using recursion and prints the result.'}\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain(\"How to write factorial program in C language?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0767deb4-d240-4d9d-88ba-823d99cf819f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context is about LangChain, a framework for developing applications powered by large language models (LLMs). However, the question is about writing a factorial program in C language, which is not related to LangChain.\n",
      "\n",
      "To answer this question, I will use my general knowledge. \n",
      "\n",
      "Here is a simple factorial program in C language:\n",
      "\n",
      "```c\n",
      "#include <stdio.h>\n",
      "\n",
      "// Function to calculate the factorial of a number\n",
      "int factorial(int n) {\n",
      "    if (n == 0) {\n",
      "        return 1;\n",
      "    } else {\n",
      "        return n * factorial(n-1);\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int num;\n",
      "    printf(\"Enter a number: \");\n",
      "    scanf(\"%d\", &num);\n",
      "    int result = factorial(num);\n",
      "    printf(\"Factorial of %d = %d\\n\", num, result);\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "This program defines a function `factorial` that calculates the factorial of a given number using recursion. The `main` function prompts the user to enter a number, calculates the factorial using the `factorial` function, and prints the result.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain(\"How to write factorial program in C language?\")['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de9ca47-bb26-403f-acce-c8c022c6588b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "16f0645f-93b8-41b8-a6f7-c7db947b3ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'what is langchain?', 'context': [Document(id='c2810ac1-af4b-4de2-8d62-e7323f9b2012', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:'), Document(id='11941896-5149-4816-baab-202238c5b2d8', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\")], 'answer': 'LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies every stage of the LLM application lifecycle, including development, productionization, and deployment.'}\n",
      "[Document(id='c2810ac1-af4b-4de2-8d62-e7323f9b2012', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:'), Document(id='11941896-5149-4816-baab-202238c5b2d8', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\")]\n"
     ]
    }
   ],
   "source": [
    "#Step-1 load the data\n",
    "loader = TextLoader('my_docs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "#Step-2 split - chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "#Step-3 embeddingobject\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "#Step-4 stores to vectorDB\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "#Step-5 similarity_search ==> retriver_object\n",
    "#vectorstore.similarity_search(\"what is langchain?\")\n",
    "retriever_obj = vectorstore.as_retriever() # Convert vector stores into a retriver object\n",
    "\n",
    "#Step-6: create llm objct\n",
    "llm_obj = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "# Hybrid RAG\n",
    "my_template=\"\"\"\n",
    "    Use the follwoing context to answer the question.\n",
    "    If the answer is not present in the context, answer from your general knowledge.\n",
    "     Context:\n",
    "     {context}\n",
    "     Question:\n",
    "     {input}\n",
    "     Answer:\"\"\"\n",
    "my_prompt = PromptTemplate(input_variables=[\"context\",\"input\"],template=my_template)\n",
    "\n",
    "stuff_chain  = create_stuff_documents_chain(llm=llm_obj,prompt=my_prompt)\n",
    "rag_chain = create_retrieval_chain(retriever=retriever_obj,combine_docs_chain=stuff_chain)\n",
    "\n",
    "response = rag_chain.invoke({'input':'what is langchain?'})\n",
    "print(response)\n",
    "print(response['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60673b76-7b3f-4779-b89b-ded7f5c1e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG with ChatHistory\n",
    "\n",
    "#  Step 1: Load pdf\n",
    "\n",
    "#  Step 2: split\n",
    "\n",
    "#  Step 3: Embeddings \n",
    "\n",
    "#  Step 4: Vector Store\n",
    "\n",
    "#  Step 5: LLM\n",
    "\n",
    "#  Step 6: Prompt with history\n",
    "\n",
    "#  Step 7: Build stuff + Retrieval Chain\n",
    "\n",
    "#  Step 8: Add RunnableWithMessageHistory \n",
    "\n",
    "    rag_with_hisotry = RunnableWithMessageHistory(rag_chain,myhistory_function,input_messages_key=\"input\")\n",
    "\n",
    "# Step 9: Create session ID and invoke rag with chatHistory\n",
    "\n",
    "#########################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1996af9-a962-4219-8ad4-33ecc01e39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8010032b-4cb8-4551-9040-f84ec263d2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_obj = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e11a079c-93c8-426b-8512-0a31d4f5a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20defc57-df25-49ba-a8f6-28806cccddcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Error in RootListenersTracer.on_chain_end callback: KeyError('output')\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response-1: {'input': 'What is attention in transformer?', 'history': [], 'context': [Document(id='af62931d-b473-4ea3-ba9c-d51bdf7e52a1', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-'), Document(id='b4d696b2-353f-4258-9418-d298fb3e1267', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and'), Document(id='aca4f21f-7105-4b04-ab92-d838968548b6', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='language modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture'), Document(id='3036c60b-bfd4-4ce2-bef3-97d98d427b0c', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='i ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:')], 'answer': 'In the context of the Transformer model, attention refers to a mechanism called \"multi-head self-attention\" or simply \"multi-head attention.\" This is a key component of the Transformer architecture, allowing the model to weigh the importance of different input elements relative to each other.\\n\\nIn more detail, attention in the Transformer is a process that allows the model to focus on specific parts of the input sequence when generating the output. This is achieved through a weighted sum of the input elements, where the weights are learned during training.\\n\\nIn the Transformer, attention is used in three different ways:\\n\\n1. Self-attention: The model attends to different positions within the input sequence itself.\\n2. Encoder-decoder attention: The encoder attends to different positions within the input sequence, and the decoder attends to different positions within the output sequence.\\n3. Multi-head attention: The model uses multiple attention mechanisms in parallel, allowing it to capture different aspects of the input sequence.\\n\\nThe Transformer uses multi-head attention to compute representations of the input and output, relying entirely on self-attention without using sequence-aligned RNNs or convolution.'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: KeyError('output')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response-2: {'input': 'Explain again in shortly', 'history': [], 'context': [Document(id='360456fb-e7ed-47ab-b1e5-da0390582df0', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been'), Document(id='048b0e21-3502-40d1-95dd-9fc0d70d7e8f', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.'), Document(id='6efd3aa6-e32a-49de-8266-00139a390d66', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-'), Document(id='4dc62e77-b905-4066-8469-aa0ace70bacd', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='architectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer')], 'answer': \"Self-attention is a mechanism that helps a sequence model understand its own sequence by relating different positions. It's used to improve computational efficiency while maintaining model performance. However, it still has a fundamental constraint of sequential computation, which limits parallelization.\"}\n"
     ]
    }
   ],
   "source": [
    "## Step-1 Load pdf file\n",
    "loader = PyPDFLoader('attention.pdf')\n",
    "documents = loader.load()\n",
    "\n",
    "## Step-2 split the docs \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "## Step-3 Embeddings \n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "## Step-4 Vectorstores\n",
    "vector_store = FAISS.from_documents(docs,embeddings)\n",
    "retriever_obj = vector_store.as_retriever()\n",
    "\n",
    "## Step-5 LLM object\n",
    "llm_obj = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "##  Step 6: Prompt with history\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Use the following context and chat history to answer the question.\n",
    "\n",
    "Chat history:\n",
    "{history}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{input}\n",
    "\"\"\")\n",
    "\n",
    "## Step 7: Build stuff + retrieval chain\n",
    "qa_chain = create_stuff_documents_chain(llm_obj,prompt)\n",
    "rag_chain = create_retrieval_chain(retriever_obj,qa_chain)\n",
    "\n",
    "## Step 8: Add RunnableWithMessageHistory \n",
    "stores = {}\n",
    "def get_session_history(session_id: str) ->BaseChatMessageHistory:\n",
    "    if session_id not in stores:\n",
    "        stores[session_id] = ChatMessageHistory()\n",
    "    return stores[session_id]\n",
    "\n",
    "rag_with_history = RunnableWithMessageHistory(rag_chain,get_session_history,input_messages_key='input',history_messages_key='history')\n",
    "\n",
    "\n",
    "## Step 9: Create session ID and invoke rag with chatHistory\n",
    "\n",
    "session_id = \"session-1\"\n",
    "query1 = \"What is attention in transformer?\"\n",
    "\n",
    "response1 = rag_with_history.invoke({\"input\":query1},config={\"configurable\":{\"session_id\":session_id}})\n",
    "\n",
    "print(\"response-1:\",response1)\n",
    "print('')\n",
    "query2 = \"Explain again in shortly\"\n",
    "response2 = rag_with_history.invoke({\"input\":query2},config={\"configurable\":{\"session_id\":session_id}})\n",
    "\n",
    "print(\"response-2:\",response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c0df40a-4ba0-413b-b3ad-30b5d22424c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Error in RootListenersTracer.on_chain_end callback: KeyError('output')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response-3: {'input': 'What did i ask previously?', 'history': [], 'context': [Document(id='cd687a9f-d4db-4c0b-93da-bcc51acadbe0', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>'), Document(id='61eb029e-809e-49dd-8d36-cd2d7c838e08', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='[10], consuming the previously generated symbols as additional input when generating the next.\\n2'), Document(id='665087ea-b71e-4ca1-83d4-5977bdb857e0', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of'), Document(id='eed52c0b-c1fe-40f2-8e59-ce24dd90ab3a', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building')], 'answer': 'You did not ask any question previously in this chat. This conversation started with a provided context and chat history, but there were no questions asked before your current one.'}\n"
     ]
    }
   ],
   "source": [
    "query3=\"What did i ask previously?\"\n",
    "response3 = rag_with_history.invoke({\"input\":query3},config={\"configurable\":{\"session_id\":session_id}})\n",
    "\n",
    "print(\"response-3:\",response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22b9373e-7953-44ac-8d40-9b4a81f33075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Error in RootListenersTracer.on_chain_end callback: KeyError('output')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response-4: {'input': 'What did i ask previously?', 'history': [], 'context': [Document(id='cd687a9f-d4db-4c0b-93da-bcc51acadbe0', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>'), Document(id='61eb029e-809e-49dd-8d36-cd2d7c838e08', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='[10], consuming the previously generated symbols as additional input when generating the next.\\n2'), Document(id='665087ea-b71e-4ca1-83d4-5977bdb857e0', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of'), Document(id='eed52c0b-c1fe-40f2-8e59-ce24dd90ab3a', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building')], 'answer': \"You didn't ask anything previously. This is the start of our conversation, and I'm ready to help with any questions or topics you'd like to discuss.\"}\n"
     ]
    }
   ],
   "source": [
    "session_id=\"session-2\"\n",
    "query3=\"What did i ask previously?\"\n",
    "response4 = rag_with_history.invoke({\"input\":query3},config={\"configurable\":{\"session_id\":session_id}})\n",
    "\n",
    "print(\"response-4:\",response4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c09b6c70-2adf-455d-9792-00e8c818ae19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session-1': InMemoryChatMessageHistory(messages=[]),\n",
       " 'session-2': InMemoryChatMessageHistory(messages=[])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b201fb20-aa73-40d4-bbde-c2b6c24bec9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(stores[\"session-1\"].messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1775ac46-ce52-4ea1-b1d2-9cf7eaa2bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Error in RootListenersTracer.on_chain_end callback: KeyError('output')\n"
     ]
    }
   ],
   "source": [
    "session_id = \"session-1\"\n",
    "query1 = \"What is attention in transformer?\"\n",
    "\n",
    "response1 = rag_with_history.invoke({\"input\":query1},config={\"configurable\":{\"session_id\":session_id}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77a94ea3-27f5-479e-80bd-9b9bd15bb958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is attention in transformer?',\n",
       " 'history': [],\n",
       " 'context': [Document(id='af62931d-b473-4ea3-ba9c-d51bdf7e52a1', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-'),\n",
       "  Document(id='b4d696b2-353f-4258-9418-d298fb3e1267', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and'),\n",
       "  Document(id='aca4f21f-7105-4b04-ab92-d838968548b6', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='language modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture'),\n",
       "  Document(id='3036c60b-bfd4-4ce2-bef3-97d98d427b0c', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='i ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:')],\n",
       " 'answer': \"In the context of the Transformer model, attention refers to the mechanism of focusing on different parts of the input or output sequence when generating representations. Specifically, it's a method of computing representations of the input and output without relying on sequence-aligned RNNs or convolution.\\n\\nIn the Transformer architecture, attention is implemented as multi-head self-attention, which is used in three different ways:\\n\\n1. Multi-head self-attention in the encoder\\n2. Multi-head self-attention in the decoder\\n3. Multi-head attention in the encoder-decoder interaction\\n\\nMulti-head attention is an extension of the standard self-attention mechanism, which allows the model to jointly attend to information from different representation subspaces at different positions. This is achieved by splitting the model's input into multiple attention heads, each of which receives a different attention weight. The attention weights are then concatenated and linearly transformed to produce the final attention output.\\n\\nIn the Transformer model, the multi-head attention mechanism is used to compute representations of the input and output without using sequence-aligned RNNs or convolution. This allows the model to better capture long-range dependencies and contextual relationships in the input and output sequences.\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8a97073-071e-4ec2-8ab8-79c55154482d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores[\"session-1\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7019311e-21c2-4549-a511-22287c00c63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='python is a general purpose program'),\n",
       " Document(metadata={}, page_content='Langchain helps build llm apps'),\n",
       " Document(metadata={}, page_content='Ollama model works locally')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "docs = [ Document(page_content=\"python is a general purpose program\"),\n",
    "        Document(page_content=\"Langchain helps build llm apps\"),\n",
    "        Document(page_content=\"Ollama model works locally\")]\n",
    "\n",
    "docs\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b143a8c7-c84c-42df-af1d-4427cb3b78b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are helpful assistant. Use the following documents to answer the questsion.\n",
    "{context}\n",
    "Question:{input}\n",
    "Answer:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d395f54-82b3-412c-bbe6-7e680def8266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is Ollama?',\n",
       " 'context': [Document(id='c1c77dc4-6003-4965-8718-24de8df07532', metadata={}, page_content='Ollama model works locally'),\n",
       "  Document(id='bc37a780-8fc6-428e-8c2f-ed1dedc83db1', metadata={}, page_content='python is a general purpose program'),\n",
       "  Document(id='7832ce0f-0ef8-4aee-ac73-01f95d447161', metadata={}, page_content='Langchain helps build llm apps')],\n",
       " 'answer': 'Based on the provided information, Ollama appears to be a model that works locally.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = FAISS.from_documents(docs,embeddings)\n",
    "retriever_obj = vector_store.as_retriever()\n",
    "\n",
    "## Step-5 LLM object\n",
    "llm_obj = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "qa_chain = create_stuff_documents_chain(llm_obj,prompt)\n",
    "rag_chain = create_retrieval_chain(retriever_obj,qa_chain)\n",
    "\n",
    "rag_chain.invoke({\"input\":\"what is Ollama?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7da49cad-6105-4e52-942a-0656cddce1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the given documents, Ollama is an LLM model that works locally.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke({\"input\":\"what is Ollama?\"})['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44d1fff5-44d0-4a22-ac6e-8d7f87d3b721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the given information, there is no mention of Ruby. However, I can provide information about Ruby as a programming language.\\n\\nRuby is a dynamic, high-level, and general-purpose programming language. It was created in the mid-1990s by Yukihiro Matsumoto, also known as Matz. Ruby is known for its simplicity, readability, and ease of use, making it a popular choice for web development, scripting, and other applications.\\n\\nRuby is often used with the Ruby on Rails (RoR) framework, which is a popular web application framework that follows the Model-View-Controller (MVC) pattern.\\n\\nIf you have any specific questions about Ruby or its applications, I'll be happy to help.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke({\"input\":\"what is ruby?\"})['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b34125-f3f0-4f09-b4a4-aa17c98b56c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56e52d1a-8e1c-43e4-808e-4c78e9e7a602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='gemma2:2b' created_at='2025-11-27T10:23:47.1085841Z' done=True done_reason='stop' total_duration=9436024800 load_duration=6281367200 prompt_eval_count=54 prompt_eval_duration=1616869000 eval_count=25 eval_duration=1483657400 message=Message(role='assistant', content='Python is a versatile, dynamically-typed programming language renowned for its ease of learning and object-oriented approach. \\n', thinking=None, images=None, tool_name=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "import ollama \n",
    "text=\"\"\"\n",
    "python is a general purpose programming \n",
    "language. its supports dynamic type\n",
    "programming techinque.\n",
    "In python everything is class object design technique\n",
    "python is easy to learn.\"\"\"\n",
    "\n",
    "prompt= f\"Summarize this content in single sentence:{text}\"\n",
    "\n",
    "response = ollama.chat(model=\"gemma2:2b\",messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d01f3599-dd25-458a-a427-8834cf756a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a versatile, dynamically-typed programming language renowned for its ease of learning and object-oriented approach. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f79e3e-5718-49a8-81e9-df951d3fa22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "## Step 7: Build stuff + retrieval chain\n",
    "qa_chain = create_stuff_documents_chain(llm_obj,prompt)\n",
    "rag_chain = create_retrieval_chain(retriever_obj,qa_chain)\n",
    "def convert_output_to_aimessage(output):\n",
    "    return AIMessage(content=output[\"answer\"])\n",
    "final_rag_chain = rag_chain | convert_output_to_aimessage\n",
    "## Step 8: Add RunnableWithMessageHistory \n",
    "stores = {}\n",
    "def get_session_history(session_id: str) ->BaseChatMessageHistory:\n",
    "    if session_id not in stores:\n",
    "        stores[session_id] = ChatMessageHistory()\n",
    "    return stores[session_id]\n",
    "rag_with_history = RunnableWithMessageHistory(\n",
    "    final_rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3222fa3c-80b8-4455-8554-755a5c87f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Streamlit \n",
    "   |->python framework \n",
    "                 |->build interactive web apps\n",
    "\n",
    "   |-> ML \n",
    "   |-> Data visualization (ex: pandas dataframe -> chart ..)\n",
    "   |-> Data dashboard \n",
    "   |-> RAG appln\n",
    "\n",
    "   |--> NOT using HTML / CSS / JS \n",
    "   |--> simple python program\n",
    "                         |-->functionCall()\n",
    "\n",
    "   |-->Keyfeatures\n",
    "        |->UI widgets (button,textbox,fileuploaders,checkbox,radio button..)\n",
    "        |->Chart \n",
    "        |->Layout\n",
    "        |->Table \n",
    "        |->chat interface \n",
    "\n",
    "   |--> import streamlit as st\n",
    "\n",
    "        st.functionName() --->[ UserInterface ]\n",
    "\n",
    "   |--> streamlit run appFile.py (or) python -m streamlit run appFile.py\n",
    "\n",
    "   |--> jupyter => ! streamlit run appFile.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1dc8e1d-77a8-4c34-85e1-05f615bf029f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\karth'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0de1c2b-43fc-4ee0-afde-942bfa7ea9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! streamlit run Demo-3/p1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04df1dc2-7c60-4d0c-89b8-9cf0589ea13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! streamlit run RAGStream.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73cea307-6884-4c2d-b0d3-4d6ded7b16c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! streamlit run simpleRAG.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140881e-e42f-4486-a7d5-ed141350ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ End of the Day4 ####################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
